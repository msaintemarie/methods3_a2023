---
title: "Ars ANOVA: Transparency and Impact of Statistical Reporting in analysis of variance-related research"
author: "Maxime Sainte-Marie"
date: "4/10/2021"
output:
  html_document
knit: (function(inputFile, encoding) {
  browseURL(
    rmarkdown::render(
      inputFile,
      encoding = encoding,
      output_file = "methodological_note.html",
      output_dir = '../manuscripts/'))})
bibliography: '../data/bibliography.bib'
header-includes:
  - \usepackage{indentfirst}
  - \usepackage{booktabs}
editor_options: 
  markdown: 
    wrap: 72
---

```{=html}
<style>
.main-container {
width: 100%;
max-width: unset;
}

h4.date{
margin-bottom: 10vh;
}

body{
font-family: Helvetica;
font-size: 1.25vw;
margin: 5vh;
}

h1.title, h4.author, h4.date {
text-align: center;
}

h4.author, h4.date {
font-size: 1.75vw
}

p {
margin: 2vh;
}

</style>
```
```{r setup, include = FALSE, echo = FALSE, message = FALSE}
set_workspace <- function(){
  
  r = getOption("repos")
  r["CRAN"] = "http://cran.us.r-project.org"
  options(repos = r, width = 150)
  
  
  # Specify the packages to be used in this sampling procedure
  packages <- c('magrittr',
                'ggplot2',
                'ggridges',
                'ggtext',
                'data.table',
                'scales',
                'lemon',
                'UpSetR',
                'ComplexUpset',
                'remotes',
                'grid',
                'fitdistrplus',
                'MASS',
                'bestNormalize',
                'devtools',
                'RBesT',
                'ggthemes',
                'rstudioapi',
                'ggpubr',
                'reticulate',
                'here',
                'mixtools',
                'patchwork')
  #Define new packages as uninstalled packages
  new_packages <-
    packages[!(packages %in% installed.packages()[, "Package"])]
  
  # Install new packages
  if (length(new_packages)){
    install.packages(new_packages)}
  #Sys.setenv(RETICULATE_PYTHON = "/usr/local/bin/python3")
  library(magrittr)
  library(ggplot2)
  library(data.table)
  library(rstudioapi)
  reticulate::use_python('/usr/bin/python3')
  
  if(!'mas3321' %in% installed.packages()[, 'Package']){
    install.packages("mas3321", repos="http://R-Forge.R-project.org")}
  
  if(!'dampack' %in% installed.packages()[, 'Package']){
    remotes::install_github("DARTH-git/dampack")}
  
  if(!'ggh4x' %in% installed.packages()[, 'Package']){
    devtools::install_github("teunbrand/ggh4x")}
  
  knitr::opts_chunk$set(echo = TRUE, fig.width=12, fig.height=8)
  here::i_am("scripts/get_anova_estimations.Rmd")
}
set_workspace()

```

# Methods

## Data Collection

Article metadata for the corpus used in this research was obtained through various sources. All articles and reviews published in 2015 and indexed in the Web of Science Core Collection (1577773 articles) were initially considered [@birkle2020web]. From that subset, only articles written in English (1532971 articles) and with DOIs (1453736 articles) were extracted from the database on August 15 2022, along with citation data. Following this, articles were assigned subdisciplines and disciplines on the basis of the journal-based disciplinary classification used by the National Science Foundation's (NSF) in its Science and Engineering Indicators reports; articles published in non-classified journals were ignored and all remaining articles (1442748) were regrouped under three higher-level disciplinary groups, namely Life Sciences, Physical Sciences, and Social Sciences & Humanities (SSH). In addition, Scimago journal ranking data was used in order to classify articles by publisher and best registered quartile; only articles from journals whose publisher and best quartile information were available were considered for further analysis (1383676 articles).

Using DOIs and publisher info obtained through the above-mentioned process, full text data for all articles was then obtained using three different web services:

- The Elsevier Full Text API ([https://www.elsevier.com/about/policies/text-and-data-mining](https://www.elsevier.com/about/policies/text-and-data-mining))
- The Crossref API ([https://www.crossref.org/documentation/retrieve-metadata/rest-api/text-and-data-mining-for-researchers/](https://www.crossref.org/documentation/retrieve-metadata/rest-api/text-and-data-mining-for-researchers/))
- The Unpaywall database ([https://unpaywall.org/products/snapshot](https://unpaywall.org/products/snapshot))

All XML files obtained through Elsevier Full Text API were parsed in order to extract both content and structure of all article bodies, with the exception of figures, tables, their captions, and their content; of the 352387 collected articles, 342800 article bodies and 350581 abstracts were successfully parsed. The online (HTML) version of articles published in Wiley journals was obtained using URLs obtained from Crossref API querying; 116190 articles were collected, out of which 113751 articles bodies and 111815 abstracts were successfully parsed. For all remaining articles, article URLs were obtained by querying either the Crossref API or the Unpaywall database; in the latter case, only articles listed as green open access were considered in order to exclude pre-print manuscripts from the database. Finally, all collected PDF files were converted into XML format using the *GeneRation Of BIbliographic Data* (GROBID) libary [@lopez2009; @romary2015; @GROBID], and each extracted file was then parsed for both article and abstract structure and content; of the 574349 articles collected, 568879 article bodies and 545359 abstracts were successfully parsed.

Following this collection process, each extracted paragraph of each extracted section of each article body and abstract was broken into sentences using the python package *sentence-splitter* [https://github.com/mediacloud/sentence-splitter](https://github.com/mediacloud/sentence-splitter), an extension of the *Lingua: sentence* Perl module [https://metacpan.org/pod/Lingua::Sentence](https://metacpan.org/pod/Lingua::Sentence) developed by Philipp Koehn and Josh Schroeder for processing the Europarl corpus [@koehn2005]. Each segmented sentence was added to be database as a distinct entry, along with the corresponding UT code, section number, section title, section label, paragraph number, sentence number, and starting sentence character rank. For each subdiscipline of each discipline of each disciplinary group, separate files were created for abstract and body information, a SQL database was created on a local server by merging all resulting files along with their initial metadata.

The global Web of Science coverage of the resulting article database is shown in the following figure, grouped by disciplinary group, discipline, and SJR best quartile.

```{r plot_coverage, echo = FALSE, message = FALSE, fig.align = 'center'}

load_wos_coverage <- function(){
  data <-
    data.table::fread(here::here('data/print', 'wos_coverage.txt')) %>%
    dplyr::rename(., best_quartile = 'sjr_best_quartile',
                  wos_articles = 'total_articles') %>%
    .[, subdisc := stringr::str_replace(subdisc, 'Miscellaneous', 'General')] %>%
    .[, disc := stringr::str_replace(disc, ' and ', ' & ')] %>%
    .[, subdisc := stringr::str_replace(subdisc, ' and ', ' & ')] %>%
    .[, subdisc := stringr::str_replace(subdisc, 'General Zoology', 'Zoology')] %>%
    .[, subdisc := stringr::str_replace(subdisc, 'General Engineering & Technology',
                                        'General Engineering')] %>%
    .[, .(parsed_articles = sum(parsed_articles), wos_articles = sum(wos_articles)),
      by = .(disc_group, disc, subdisc, best_quartile)]
  return(data)
}

plot_wos_coverage <- function(load = TRUE){
  if(file.exists(here::here('figures', 'plot_wos_coverage.jpg')) & load == TRUE){
    knitr::include_graphics(here::here('figures', 'plot_wos_coverage.jpg'))
  }
  else{
    data <- load_wos_coverage()
    plot_data <- load_wos_coverage() %>%
      .[, disc := dplyr::recode(
        disc,
        `Biomedical Research` = 'Biomedical\nResearch',
        `Engineering and Technology` = 'Engineering&\nTechnology',
        `Professional Fields` = 'Professional\nFields',
        `Clinical Medicine` = 'Clinical\nMedicine',
        `Earth and Space` = 'Earth&\nSpace',
        `Social Sciences` = 'Social\nSciences')] %>%
      .[, disc_group := dplyr::recode(
        disc_group,
        `Life Sciences` = paste0('Life Sciences:\n',
                                 data[disc_group == 'Life Sciences',
                                      sum(parsed_articles)],
                                 ' (',
                                 data[disc_group == 'Life Sciences',
                                      round((sum(parsed_articles)/sum(wos_articles)) *100, 2)],
                                 '%)'),
        `Physical Sciences` = paste0('Physical Sciences:\n',
                                     data[disc_group == 'Physical Sciences',
                                          sum(parsed_articles)],
                                     ' (',
                                     data[disc_group == 'Physical Sciences',
                                          round((sum(parsed_articles)/sum(wos_articles)) *100, 2)],
                                     '%)'),
        SSH = paste0('SSH:\n',
                     data[disc_group == 'SSH',
                          sum(parsed_articles)],
                     ' (',
                     data[disc_group == 'SSH',
                          round((sum(parsed_articles)/sum(wos_articles))*100, 2)],
                     '%)')
      )]
    prop_parsed <- data %>%
      .[, round(sum(parsed_articles)/
                  sum(wos_articles) * 100, 2)]
    plot <- plot_data %>%
      .[,.(parsed_articles = sum(parsed_articles),
           wos_articles = sum(wos_articles)),
        by = c('disc_group',
               'disc',
               'best_quartile')] %>%
      .[, prop_parsed := (parsed_articles/wos_articles) * 100] %>%
      ggplot2::ggplot(
        data = .,
        aes(x = reorder(disc, dplyr::desc(disc)),
            y = prop_parsed,
            color = best_quartile)) +
      ggplot2::coord_flip() +
      scale_color_manual(
        name = 'Best Quartile',
        values = c('#000000', '#F0E442', '#009E73', '#D55E00'),
        labels = c(paste0('**Q1**: ', data[best_quartile == 'Q1', sum(parsed_articles)],
                          ' (', data[best_quartile == 'Q1',
                                     round((sum(parsed_articles)/sum(wos_articles))*100, 2)],
                          '%)'),
                   paste0('**Q2**: ', data[best_quartile == 'Q2', sum(parsed_articles)],
                          ' (', data[best_quartile == 'Q2',
                                     round((sum(parsed_articles)/sum(wos_articles))*100, 2)],
                          '%)'),
                   paste0('**Q3**: ', data[best_quartile == 'Q3', sum(parsed_articles)],
                          ' (', data[best_quartile == 'Q3',
                                     round((sum(parsed_articles)/sum(wos_articles))*100, 2)],
                          '%)'),
                   paste0('**Q4**: ', data[best_quartile == 'Q4', sum(parsed_articles)],
                          ' (', data[best_quartile == 'Q4',
                                     round((sum(parsed_articles)/sum(wos_articles))*100, 2)],
                          '%)'))) +
      geom_point(size = 3) +
      labs(
        x='',
        y = 'WoS Proportion (%)',
        title = paste0(data[, sum(parsed_articles)],
                       ' articles parsed in total (',
                       prop_parsed,
                       '%)')) +
      theme(axis.text.x = element_text(size=12, face = 'bold'),
            axis.title.x = element_text(size=13, face = 'bold'),
            axis.text.y = element_text(size=12),
            plot.title =
              element_text(hjust = 0.5, size = 16, face = 'bold'),
            plot.subtitle =
              element_text(hjust = 0.5, size = 14, face = 'bold'),
            strip.text = element_text(face = 'bold', size = 14),
            strip.background=element_blank(),
            legend.text= ggtext::element_markdown(size=14),
            legend.title=element_text(size=14),
            legend.position = 'top',
            legend.margin=margin(0,0,0,0),
            legend.box.margin=margin(-5,-5,-5,-5),
            legend.key = element_blank(),
            panel.grid.major.y =
              element_line(colour = 'grey90'),
            panel.grid.major.x =
              element_line(colour = 'grey90'),
            panel.background =
              element_rect(fill = 'white', colour = 'black')) +
      ggh4x::facet_grid2(disc_group ~ .,
                         scales = 'free',
                         space = 'free_y',
                         independent = 'x') +
      ggrepel::geom_text_repel(
        aes(label = parsed_articles),
        size = 3,
        box.padding = 0.65,
        point.padding= 0.65,
        show.legend = FALSE,
        color = 'grey60')
    ggplot2::ggsave(filename = 'plot_wos_coverage.jpg',
                    plot = plot,
                    height = 8,
                    width = 12,
                    path = here::here('figures'),
                    dpi = 600)
  }
}

plot_wos_coverage()


```
While the database contains most of 2015 WoS entries, important differences can be observed between disciplinary groups, disciplines, as well as quartiles. As regards to the latter, a general trend can be observed regarding online availability, as most of the database content originates from articles published in upper-quartile journals, regardless of discipline. Also, SSH disciplines tend to be underrepresented in the database compared to Life Sciences and Physical Sciences disciplines, the latter containining a majority of articles for most quartiles. Finally, while the database contains a higher proportion of Life Science articles than Physical Sciences, WoS coverage for the different quartiles of each discipline displays higher variance in the former group.

## Regex Parsing

In order to identify papers that use terms related to analysis of variance, homoscedasticity, and multiplicity, various regular expressions were designed and applied to the corpus. A first set of regexes was aimed at identifying articles containing orthographic variants of expressions related to various analysis of variance tests. The first regular expression aims to capture all mentions of parametric analysis of variance in the corpus. Negative lookaheads and lookbehinds were added to the regex in order to exclude all cases referring to non-parametric considerations. A second regular expression aims to catch expressions referring to the following non-parametric analysis of variance tests: Jonckheere’s trend test, Friedman test, Kruskal-Wallis test, permutational ANOVA, non-parametric ANOVA as well as ANOVA on ranks. The last two regexes were designed to capture strings related to homoscedasticity and multiplicity considerations respectively. In the first case, references to either hetero- or homoscedasticity were considered relevant for the present study, hence the ulterior use of the expression ‘scedasticity’ to refer to such cases. This scedasticity regex was designed in order to capture variants of the following expressions: homoscedasticity, heteroscedasticity, equal/similar/comparable variance, Levene’s test, Bartlett’s test, Hartley’s fMax, Glejser test, White test, Cochran’s C test, Box’s M test, Breusch-Pagan test, Goldfeld-Quandt test, Brown- Forsythe test, Cook-Weisberg test, Harrison-McCabe test, Fligner-Killeen test. As for multiplicity considerations, variants of the following strings were captured: Bonferroni correction, family-wise error rate (FWER), false discovery rate (FDR), Dunn’s test, Dunnett’s test, Šidák correction, Tukey’s range test, Holm-Bonferroni step-down procedure, and Hochberg’s step-up procedure. In the case of ANOVA-related expressions referring to last names, regular expressions were designed in order to ignore citations; this was done by excluding all expressions followed by substrings characteristic of various citation formats, namely the abbreviation et al., digits surrounded by brackets or parentheses as well as the conjunction and followed by a proper noun followed by digits surrounded by brackets or parentheses, as in Smith (2008) or Johnson [3]. Application of these regex to the corpus resulted in the extraction of 178818 expressions distributed over 55746 articles and four different regex capture groups: Parametric, Nonparametric, Homoscedasticity, Multiplicity. While a single article may contain one or many expressions from the same or different capture groups, only binary membership relations between articles and capture groups are considered for this study. In other words, each capture group set includes every article which contains at least one expression belonging to that capture group, while each article is a member of every capture group set whose defining regular expression captures at least one of its substrings.

The following upset plot shows the distribution of articles over the various regex capture set configurations that are relevant for the present research. The matrix in the lower part of the plot shows the different set intersection configurations that include the two ANOVA test regex capture sets. Each row of the matrix stands for a distinct regex capture group, and the number of articles included in each regex capture group is indicated alongside the group name. For visualisation and analysis purposes, these configurations are grouped by test type (Parametric, Non-Parametric, Both) and ascending intersection degree, i.e. the number of regex capture group sets; article and intersection sets that do not include one or both ANOVA test regex capture groups (Parametric, Non-Parametric) were not included in the plot. Stacked lollipop charts in the upper part of the plot shows article frequencies for the different journal quartile and disciplinary group subsets. Since the frequency distribution of articles over these two attributes can differ by several orders of magnitude, all figures have been plotted on logarithmic axes, with quartile and disciplinary group frequencies for each regex capture group intersection set shown respectively above and aside each other. Horizontal bars above each group of stacked lollipops indicate the number of articles included in the whole group, and article frequencies for each quartile and disciplinary group subset are shown in the two legends, alongside their corresponding labels.

```{r plot_regex_matches, echo = FALSE, message = FALSE, warning=FALSE}

presence = ComplexUpset:::get_mode_presence('exclusive_intersection')

summarise_intersection_values = function(df) {
  aggregate(
    as.formula(paste0(presence, ' ~ intersection + disc_group + Quartile')),
    df,
    FUN=sum
  )
}

summarise_raw_intersection_values = function(df) {
  aggregate(
    as.formula(paste0(presence, ' ~ intersection')),
    df,
    FUN=sum
  )
}

get_set_size_data <- function(){
  data <-
    data.table::fread(here::here('data', 'regex_captures.txt')) %>%
    .[, regex_type := dplyr::recode(regex_type,
                                    manova = 'anova')] %>%
    .[, .(expression = paste0(expression, collapse = ';')),
      by = .(ut, regex_type)] %>%
    data.table::dcast(., ut ~ regex_type,
                      value.var = 'expression') %>%
    merge(.,
          get_harvested_articles(),
          by = 'ut',
          all.y = TRUE) %>%
    unique(.) %>%
    .[, .(
      disc_group,
      Quartile = best_quartile,
      Parametric = as.numeric(!is.na(anova)),
      Nonparametric = as.numeric(!is.na(nonpar)),
      Multiplicity = as.numeric(!is.na(mult)),
      Homoscedasticity = as.numeric(!is.na(sced))
    )] %>%
    .[, lapply(.SD, sum, na.rm = T), by = .(disc_group, Quartile)] %>%
    melt(., id.vars = c('disc_group', 'Quartile'), measure.vars = c('Parametric', 'Nonparametric', 'Multiplicity', 'Homoscedasticity'), variable.name = 'group', value.name = 'count')
  return(data)
}

plot_regex_matches <- function(load = TRUE){
  if(file.exists(here::here('figures', 'plot_regex_matches.jpg')) & load == TRUE){
    knitr::include_graphics(here::here('figures', 'plot_regex_matches.jpg'))
  }
  else{
    set_data <- get_set_size_data()
    data <-
      data.table::fread(here::here('data', 'regex_captures.txt')) %>%
      .[, regex_type := dplyr::recode(regex_type,
                                      manova = 'anova')] %>%
      .[, .(expression = paste0(expression, collapse = ';')),
        by = .(ut, regex_type)] %>%
      data.table::dcast(., ut ~ regex_type,
                        value.var = 'expression') %>%
      merge(.,
            get_harvested_articles(),
            by = 'ut',
            all.y = TRUE) %>%
      unique(.) %>%
      .[, .(
        disc_group,
        Quartile = best_quartile,
        Parametric = as.numeric(!is.na(anova)),
        Nonparametric = as.numeric(!is.na(nonpar)),
        Multiplicity = as.numeric(!is.na(mult)),
        Homoscedasticity = as.numeric(!is.na(sced))
      )]
    
    par_plot <- ComplexUpset::upset(
      data,
      rev(list(
        'Parametric',
        'Nonparametric',
        'Multiplicity',
        'Homoscedasticity')),
      name = 'Parametric ANOVA Tests',
      sort_intersections = F,
      sort_sets = F,
      height_ratio = 0.3,
      intersections= list('Parametric',
                          c('Parametric', 'Multiplicity'),
                          c('Parametric', 'Homoscedasticity'),
                          c('Parametric', 'Multiplicity', 'Homoscedasticity')
      ),
      base_annotations = list(
        'Lollipop counts'=(
          ggplot()
          + geom_linerange(data = summarise_intersection_values,
                           aes(x=intersection, ymin = 0, ymax = !!presence,
                               group = disc_group,
                               linetype = disc_group,
                               color = forcats::fct_rev(Quartile)),
                           stat = 'identity', na.rm = T,
                           position = position_dodge(width = 0.65),
                           linewidth = 1,
                           key_glyph = 'path'
          )
          + geom_point(data = summarise_intersection_values,
                       aes(y = !!presence,  group = disc_group,
                           color = forcats::fct_rev(Quartile)),
                       position_dodge(width = 0.65),
                       stat = 'identity',  na.rm = T, size = 2)
          + geom_errorbar(data = summarise_raw_intersection_values,
                          aes(ymin = !!presence, ymax = !!presence),
                          stat = 'identity', na.rm = T,
                          color = 'grey20')
          + geom_text(data = summarise_raw_intersection_values,
                      aes(y = !!presence, x = intersection, label = !!presence),
                      vjust = -0.25)
          + scale_y_continuous(trans = scales::pseudo_log_trans(base = 10, sigma = 6),
                               breaks = c(0, 10, 100, 1000, 10000, 100000),
                               expand = c(0,0),
                               limits = c(0, 100000),
                               labels = scales::label_number()
          )
          + scale_color_manual(values = c('#073642', '#b58900', '#268bd2', '#dc322f'),
                               labels = c(paste0('Q4 (',
                                                 set_data[Quartile == 'Q4',
                                                          sum(count)], ')'),
                                          paste0('Q3 (',
                                                 set_data[Quartile == 'Q3',
                                                          sum(count)], ')'),
                                          paste0('Q2 (',
                                                 set_data[Quartile == 'Q2',
                                                          sum(count)], ')'),
                                          paste0('Q1 (',
                                                 set_data[Quartile == 'Q1',
                                                          sum(count)], ')')))
          + scale_linetype_manual(values = c('solid', 'dotdash', 'dotted'),
                                  labels = c(paste0('Life Sciences (',
                                                    set_data[disc_group == 'Life Sciences',
                                                             sum(count)], ')'),
                                             paste0('Physical Sciences (',
                                                    set_data[disc_group == 'Physical Sciences',
                                                             sum(count)], ')'),
                                             paste0('SSH (',
                                                    set_data[disc_group == 'SSH',
                                                             sum(count)], ')')))
          + labs(color = 'Quartile', linetype = 'Disciplinary Group')
          + theme(panel.grid.minor = element_blank(),
                  axis.title.y = element_blank(),
                  legend.title = element_text(face = 'bold'))
          + guides(color = guide_legend(override.aes=list(linetype = 0),
                                        reverse = T,
                                        title.position = 'top',
                                        title.hjust = 0.5),
                   linetype = guide_legend(title.position = 'top',
                                           title.hjust = 0.5)))),
      matrix = ComplexUpset::intersection_matrix(segment = geom_segment(linetype = 0)),
      stripes = ComplexUpset::upset_stripes(
        geom = geom_segment(linewidth=14),
        colors=c('white', 'white', 'grey90', 'grey90')),
      set_sizes = FALSE,
      labeller=ggplot2::as_labeller(c(
        'Parametric'=
          paste0('Parametric (', set_data[group == 'Parametric', sum(count)], ')'),
        'Nonparametric'=
          paste0('Nonparametric (', set_data[group == 'Nonparametric', sum(count)], ')'),
        'Multiplicity' =
          paste0('Multiplicity (', set_data[group == 'Multiplicity', sum(count)], ')'),
        'Homoscedasticity' =
          paste0('Homoscedasticity (', set_data[group == 'Homoscedasticity', sum(count)], ')')
      ))
    )
    
    nonpar_plot <- ComplexUpset::upset(
      data,
      rev(list(
        'Parametric',
        'Nonparametric',
        'Multiplicity',
        'Homoscedasticity')),
      name = 'Nonparametric ANOVA Tests',
      sort_intersections = F,
      sort_sets = F,
      height_ratio = 0.3,
      intersections= list('Nonparametric',
                          c('Nonparametric', 'Multiplicity'),
                          c('Nonparametric', 'Homoscedasticity'),
                          c('Nonparametric', 'Multiplicity', 'Homoscedasticity')
      ),
      base_annotations = list(
        'Lollipop counts'=(
          ggplot()
          + geom_linerange(data = summarise_intersection_values,
                           aes(x=intersection, ymin = 0, ymax = !!presence,
                               group = disc_group,
                               linetype = disc_group,
                               color = forcats::fct_rev(Quartile)),
                           stat = 'identity', na.rm = T,
                           position = position_dodge(width = 0.65),
                           linewidth = 1,
                           key_glyph = 'path'
          )
          + geom_point(data = summarise_intersection_values,
                       aes(y = !!presence,  group = disc_group,
                           color = forcats::fct_rev(Quartile)),
                       position_dodge(width = 0.65),
                       stat = 'identity',  na.rm = T, size = 2)
          + geom_errorbar(data = summarise_raw_intersection_values,
                          aes(ymin = !!presence, ymax = !!presence),
                          stat = 'identity', na.rm = T,
                          color = 'grey20')
          + geom_text(data = summarise_raw_intersection_values,
                      aes(y = !!presence, x = intersection, label = !!presence),
                      vjust = -0.25)
          + scale_y_continuous(trans = scales::pseudo_log_trans(base = 10, sigma = 6),
                               breaks = c(0, 10, 100, 1000, 10000),
                               expand = c(0,0),
                               limits = c(0, 10000),
                               labels = scales::label_number()
          )
          + scale_color_manual(values = c('#073642', '#b58900', '#268bd2', '#dc322f'),
                               labels = c(paste0('Q4 (',
                                                 set_data[Quartile == 'Q4',
                                                          sum(count)], ')'),
                                          paste0('Q3 (',
                                                 set_data[Quartile == 'Q3',
                                                          sum(count)], ')'),
                                          paste0('Q2 (',
                                                 set_data[Quartile == 'Q2',
                                                          sum(count)], ')'),
                                          paste0('Q1 (',
                                                 set_data[Quartile == 'Q1',
                                                          sum(count)], ')')))
          + scale_linetype_manual(values = c('solid', 'dotdash', 'dotted'),
                                  labels = c(paste0('Life Sciences (',
                                                    set_data[disc_group == 'Life Sciences',
                                                             sum(count)], ')'),
                                             paste0('Physical Sciences (',
                                                    set_data[disc_group == 'Physical Sciences',
                                                             sum(count)], ')'),
                                             paste0('SSH (',
                                                    set_data[disc_group == 'SSH',
                                                             sum(count)], ')')))
          + labs(color = 'Quartile', linetype = 'Disciplinary Group')
          + theme(panel.grid.minor = element_blank(),
                  axis.title.y = element_blank(),
                  legend.title = element_text(face = 'bold'))
          + guides(color = guide_legend(override.aes=list(linetype = 0),
                                        reverse = T,
                                        title.position = 'top',
                                        title.hjust = 0.5),
                   linetype = guide_legend(title.position = 'top',
                                           title.hjust = 0.5)))),
      matrix = ComplexUpset::intersection_matrix(segment = geom_segment(linetype = 0)),
      stripes = ComplexUpset::upset_stripes(
        geom = geom_segment(linewidth=14),
        colors=c('white', 'white', 'grey90', 'grey90')),
      set_sizes = F,
      labeller=ggplot2::as_labeller(c(
        'Parametric'='',
        'Nonparametric'='',
        'Multiplicity' = '',
        'Homoscedasticity' = ''
      ))
    )
    
    both_plot <- ComplexUpset::upset(
      data,
      rev(list(
        'Parametric',
        'Nonparametric',
        'Multiplicity',
        'Homoscedasticity')),
      name = 'Both ANOVA Test Types',
      sort_intersections = F,
      sort_sets = F,
      height_ratio = 0.3,
      intersections= list(c('Parametric', 'Nonparametric'),
                          c('Parametric', 'Nonparametric', 'Multiplicity'),
                          c('Parametric', 'Nonparametric', 'Homoscedasticity'),
                          c('Parametric', 'Nonparametric', 'Multiplicity', 'Homoscedasticity')
      ),
      base_annotations = list(
        'Lollipop counts'=(
          ggplot()
          + geom_linerange(data = summarise_intersection_values,
                           aes(x=intersection, ymin = 0, ymax = !!presence,
                               group = disc_group,
                               color = forcats::fct_rev(Quartile),
                               linetype = disc_group),
                           stat = 'identity', na.rm = T,
                           position = position_dodge(width = 0.65),
                           linewidth = 1,
                           key_glyph = 'path'
          )
          + geom_point(data = summarise_intersection_values,
                       aes(y = !!presence,  group = disc_group,
                           color = forcats::fct_rev(Quartile)),
                       position_dodge(width = 0.65),
                       stat = 'identity',  na.rm = T, size = 2)
          + geom_errorbar(data = summarise_raw_intersection_values,
                          aes(ymin = !!presence, ymax = !!presence),
                          stat = 'identity', na.rm = T,
                          color = 'grey20')
          + geom_text(data = summarise_raw_intersection_values,
                      aes(y = !!presence, x = intersection, label = !!presence),
                      vjust = -0.25)
          + scale_y_continuous(trans = scales::pseudo_log_trans(base = 10, sigma = 6),
                               breaks = c(0, 10, 100, 1000, 10000),
                               expand = c(0,0),
                               limits = c(0, 10000),
                               labels = scales::label_number()
          )
          + scale_color_manual(values = c('#073642', '#b58900', '#268bd2', '#dc322f'),
                               labels = c(paste0('Q4 (',
                                                 set_data[Quartile == 'Q4',
                                                          sum(count)], ')'),
                                          paste0('Q3 (',
                                                 set_data[Quartile == 'Q3',
                                                          sum(count)], ')'),
                                          paste0('Q2 (',
                                                 set_data[Quartile == 'Q2',
                                                          sum(count)], ')'),
                                          paste0('Q1 (',
                                                 set_data[Quartile == 'Q1',
                                                          sum(count)], ')')))
          + scale_linetype_manual(values = c('solid', 'dotdash', 'dotted'),
                                  labels = c(paste0('Life Sciences (',
                                                    set_data[disc_group == 'Life Sciences',
                                                             sum(count)], ')'),
                                             paste0('Physical Sciences (',
                                                    set_data[disc_group == 'Physical Sciences',
                                                             sum(count)], ')'),
                                             paste0('SSH (',
                                                    set_data[disc_group == 'SSH',
                                                             sum(count)], ')')))
          + labs(color = 'Quartile', linetype = 'Disciplinary Group')
          + theme(panel.grid.minor = element_blank(),
                  axis.title.y = element_blank(),
                  legend.title = element_text(face = "bold"))
          + guides(color = guide_legend(override.aes=list(linetype = 0),
                                        reverse = T,
                                        title.position = 'top',
                                        title.hjust = 0.5),
                   linetype = guide_legend(title.position = 'top',
                                           title.hjust = 0.5)))),
      matrix = ComplexUpset::intersection_matrix(segment = geom_segment(linetype = 0)),
      stripes = ComplexUpset::upset_stripes(
        geom = geom_segment(linewidth=14),
        colors=c('white', 'white', 'grey90', 'grey90')),
      set_sizes = F,
      labeller=ggplot2::as_labeller(c(
        'Parametric'='',
        'Nonparametric'='',
        'Multiplicity' = '',
        'Homoscedasticity' = '')
      )
    )
    plots <- (par_plot | nonpar_plot | both_plot) &
      theme(legend.position = 'top', legend.direction = 'horizontal',
            legend.background = element_rect(colour = "grey50"))
    plots <- plots + patchwork::plot_layout(guides = 'collect')
    plots <- patchwork::patchworkGrob(plots)
    left_title <- ggpubr::text_grob("# Article Matches", size = 15, face = "bold", rot = 90)
    bottom_title <- ggpubr::text_grob("Regex Match Configurations", size = 15, face = "bold")
    plots <- gridExtra::grid.arrange(plots, left = left_title, bottom = bottom_title)
    ggplot2::ggsave(filename = 'plot_regex_matches.jpg',
                    plot = plots,
                    width = 12,
                    height = 8,
                    path = here::here('figures'),
                    dpi = 600)
  }
}
plot_regex_matches()

```

As the plot shows, article frequency distributions are markedly uneven over all plotted dimensions: cardinalities for Q1, Life Science, and Parametric article sets are indeed at least one order of magnitude higher than those of other journal quartile, disciplinary group, and regex group sets respectively. However, further disaggregation of the plotted data based on the type of ANOVA test mentioned (Parametric, Nonparametric, Both) results in more balanced frequency distributions within each resulting group. Given this and in order to better understand how reporting transparency relates to assumption testing for the different ANOVA test types, all articles from the whole PRINT article database were submitted to a double partitioning. The first partition is based on the parametric/nonparametric types of ANOVA types and comprises the following four mutually disjoint and exhaustive groups: parametric, nonparametric, both, and none. The last category has been added in order to estimate ANOVA-mentioning article proportions all the different disciplinary levels. As for the second partition, it refers to the two statistical assumptions investigated here and contains the following subsets: multiplicity, homoscedasticity, both, and none.

While these operations result in a deeper, finer-grained categorical landscape, they also multiply the number of subsets to analyse, which both decreases average set cardinality and increases the number of subsets with low cardinality. To alleviate this situation and in order to optimize robustness of the results, Bayesian statistical procedures were designed and implemented in order to populate the different subsets through the use of beta-binomial and gamma-poisson conjugate mixtures built on upper-level hierarchical data.

```{r get_data, echo = FALSE, message = FALSE}

get_harvested_articles <- function(){
  data <-
    data.table::fread(here::here('data/print',
                                 'harvested_article_info.txt')) %>%
    merge(., data.table::fread(here::here('data/print',
                                          'ut_codes_cited.csv'),
                               col.names = c('ut', 'citations')),
          by = 'ut', all.x = T) %>%
    .[, citations := data.table::nafill(citations, fill = 0)]
  return(data)
}

get_wos_coverage_estimates <- function(){
  life_discs <- list(
    list(disc = 'Biology', n_total = 3130, wos_coverage_ratio = 0.84),
    list(disc = 'Biotechnology', n_total = 245, wos_coverage_ratio = 0.87)
  )
  life_discs <- data.table::setDT(plyr::ldply(life_discs, data.frame)) %>%
    .[, wos_total := round(n_total * wos_coverage_ratio)]
  life_wos <- life_discs[, sum(wos_total)]
  life_total <- life_discs[, sum(n_total)]
  wos_coverage <- list(
    list(disc_group = 'Humanities', n_total = 5067, wos_coverage_ratio = 0.23),
    list(disc_group = 'Life Sciences', n_total = 12879, wos_coverage_ratio = 0.87),
    list(disc_group = 'Physical Sciences', n_total = 18223, wos_coverage_ratio = 0.84),
    list(disc_group = 'Social Sciences', n_total = 9803, wos_coverage_ratio = 0.40)
  )
  wos_coverage <- data.table::setDT(plyr::ldply(wos_coverage, data.frame)) %>%
    .[, wos_total := round(n_total * wos_coverage_ratio)]
  ssh <-
    list(disc_group = 'SSH',
         n_total = wos_coverage[disc_group %in% c('Humanities', 'Social Sciences'),
                                sum(n_total)],
         wos_total = wos_coverage[disc_group %in% c('Humanities', 'Social Sciences'),
                                  sum(wos_total)])
  wos_coverage <- data.table::rbindlist(list(wos_coverage, ssh),
                                        fill = T, use.names = T) %>%
    .[disc_group %in% c('Life Sciences', 'Physical Sciences', 'SSH')] %>%
    .[disc_group == 'Life Sciences', wos_total:= wos_total + life_wos] %>%
    .[disc_group == 'Physical Sciences', wos_total:= wos_total - life_wos] %>%
    .[disc_group == 'Life Sciences', n_total:= n_total + life_total] %>%
    .[disc_group == 'Physical Sciences', n_total:= n_total - life_total] %>%
    .[, wos_coverage_ratio := wos_total / n_total]
  return(wos_coverage)
}

get_anova_data <- function(load = TRUE){
  if(file.exists(here::here('data', 'anova_data.txt')) & load == TRUE){
    data <-
      data.table::fread(here::here('data', 'anova_data.txt'),
                        sep = '\t', encoding = 'UTF-8')
  }
  else{
    data <-
      data.table::fread(here::here('data', 'regex_captures.txt')) %>%
      .[, regex_type := dplyr::recode(regex_type,
                                      manova = 'anova')] %>%
      .[, .(expression = paste0(expression, collapse = ';')),
        by = .(ut, regex_type)] %>%
      data.table::dcast(., ut ~ regex_type,
                        value.var = 'expression') %>%
      merge(., get_harvested_articles(),
            by = 'ut',
            all.y = TRUE) %>%
      unique(.) %>%
      .[, .(disc_group, disc, subdisc, best_quartile, citations,
            incremented_citations = citations + 1,
            a = !is.na(anova), n = !is.na(nonpar),
            m = !is.na(mult), s = !is.na(sced)),] %>%
      .[!(a|n), anova_type := 'none'] %>%
      .[(a & n), anova_type := 'both'] %>%
      .[(a & !n), anova_type := 'parametric'] %>%
      .[(!a & n), anova_type := 'nonparametric'] %>%
      .[!(m|s), assumption_test := 'none'] %>%
      .[(m & s), assumption_test := 'both'] %>%
      .[(m & !s), assumption_test := 'multiplicity'] %>%
      .[(!m & s), assumption_test := 'scedasticity'] %>%
      .[, c('a', 'n', 'm', 's') := c(NULL, NULL, NULL, NULL)] %>%
      .[, subdisc := stringr::str_replace(subdisc, 'Miscellaneous', 'General')] %>%
      .[, disc := stringr::str_replace(disc, ' and ', ' & ')] %>%
      .[, subdisc := stringr::str_replace(subdisc, ' and ', ' & ')] %>%
      .[, subdisc := stringr::str_replace(subdisc, 'General Zoology', 'Zoology')] %>%
      .[, subdisc := stringr::str_replace(subdisc, 'General Engineering & Technology',
                                          'General Engineering')]
    data.table::fwrite(data, here::here('data', 'anova_data.txt'), sep = '\t')
  }
  data <- data[disc != 'Arts']
  return(data)}

get_article_frequencies <- function(){
  data <- load_wos_coverage() %>%
    .[, level := 'subdisc'] %>%
    .[, field := subdisc] %>%
    .[, parsed_field_articles := sum(parsed_articles),
      by = .(subdisc)] %>%
    .[, parsed_group_quartile_articles := sum(parsed_articles),
      by = .(best_quartile, disc)]
  data <-
    data.table::rbindlist(
      list(
        data,
        data %>%
          .[, .(parsed_articles = sum(parsed_articles),
                wos_articles = sum(wos_articles)),
            by = .(best_quartile, disc_group, disc)] %>%
          .[, level := 'disc'] %>%
          .[, field := disc] %>%
          .[, parsed_field_articles := sum(parsed_articles),
            by = .(disc_group, disc)] %>%
          .[, parsed_group_quartile_articles := sum(parsed_articles),
            by = .(best_quartile, disc_group)]
      ),
      use.names = TRUE,
      fill = TRUE
    )
  data <-
    data.table::rbindlist(
      list(
        data,
        data %>%
          .[, .(parsed_articles = sum(parsed_articles),
                wos_articles = sum(wos_articles)),
            by = .(best_quartile, disc_group)] %>%
          .[, level := 'disc_group'] %>%
          .[, field := disc_group] %>%
          .[, parsed_field_articles := sum(parsed_articles),
            by = .(disc_group)] %>%
          .[, parsed_group_quartile_articles := sum(parsed_articles),
            by = .(best_quartile)]
      ),
      use.names = TRUE,
      fill = TRUE
    )
  data <- data[, c('disc_group', 'disc', 'subdisc') := .(NULL, NULL, NULL)]
  return(data)}

```

## Statistical Inferences and Analysis

Broadly defined, bayesian statistics refers to general modelling approach that relies on probability theory to quantify and update reasonable expectations or personal beliefs regarding specific states of affairs [@spiegelhalter2009bayesian].

>Unique for Bayesian statistics is that all observed and unobserved parameters in a statistical model are given a joint probability distribution, termed the prior and data distributions. The typical Bayesian workflow consists of three main steps (...): capturing available knowledge
about a given parameter in a statistical model via the prior distribution (...); determining the likelihood function using the information about the parameters available in the observed data; and combining both the prior distribution and the likelihood function using Bayes’ theorem in the form of the posterior distribution. The posterior distribution reflects one’s updated knowledge, balancing prior knowledge with observed data, and is used to conduct inferences [@van2021bayesian, 1]. 

At the heart of Bayesian inference and its criticism, prior elicitation refers to the generation of a first probabilistic model of epistemic uncertainty regarding a specific state of affairs. Traditionally, priors are determined before data collection as their name implies and are generated subjectively, that is, based on the personal judgement of the statistician or any expert(s) or relevant person(s). However, in a growing number of studies belonging to what is now called Empirical Bayesian statistics [@robbins1992empirical], prior probability distributions are not elicited through educated guesses, but rather generated from the data itself. The use of such data-dependent priors has been the subject of much debate, since these procedures lead to "double dipping", as the same dataset is used twice, namely to derive both prior and posterior distributions. However, a rather common and sound use of such "double dipping" consists in using the data at hand as an approximation to a hierarchical model [@gelman2016data; @gelman2014bayesian; @egidi2018developments; @egidi2017mixture]. Such an approximation proceeds in a top-down, two-step, fashion: data distribution at the upper level is first estimated, after which its parameters are used as prior hyperparameters to derive all lower-level distributions [@robinson2017introduction]. This use of upper-level data as prior distribution hyperparameters constrains or "shrinks" the probability spaces of the lower level subgroups, that is, curbs whatever is computed for the immediate lower level subgroups towards what has been computed at the upper level until enough lower-level evidence is accumulated to outweight upper-level information [@friston2004classical]. While having the advantage of coping for potential lack of higher granularity data, this procedure also allows for the modeling of dependency strength between subgroups, based on the idea that sibling subgroups within the data set ressemble each other though their common lineage to yield insights about each other [@efron2021computer].

As regards to the first step of the empirical hierarchical approximation procedure, hyperparameter estimation will be done using  finite parametric "mixture data-dependent" models (MDD), which allow for the modelling of hierarchical provide a highly flexible parametricclass of probability models that allow for can be used to build more realistic hierarchical models" [@gelman2014bayesian, 523].

Mixture distribution models are typically used in cases where each observation is assumed to have arisen from one of a number of different subpopulations, each of which can be modelled individually and independently by a density from a given parametric family. Finite mixture models also provide a flexible parametric alternative to non-parametric probability density estimation, by allowing multiple parametric distributions to fit distributions that are not well modelled by any standard parametric family. [@gelman2014bayesian, @stephens1997bayesian]. Finally, 



Finite mixture models are useful in providing 

The use of parametric distribution mixtures greatly facilitates the derivation of posterior distributions, as the former "always have a closed form in conjugate models and preserve the conjugacy".


If the individual mixture components are conjugate prior distributions, then updating is straightforward and the posterior distribution is also a mixture of conjugate distributions.



parametric empirical Bayes mixture models, more precisely beta-binomial and Gamma-Poisson mixtures for ratio and citation count estimations respectively.

A mixture prior for every combination of journal quartile, anova test type, and assumption test type

As regards to the dataset used here and based on its underlying disciplinary classification hierarchy, each article is part of a subdiscipline, which is part of a discipline, which is part of a disciplinary group; each article is also a member of one best journal quartile subgroup, one anova type subgroup, and one assumption test subgroup.



In order to handle the high degree of imbalance in the dataset, conjugate bayesian analytical procedures were designed using the RBesT package [@weber2021applying] in order to infer both ratio and count distributions based on reasonable expectations for each article subgroup, expectations probabilistically determined by article affiliations (quartile, disciplinary level, ...), expressed as mixtures of prior distributions, and updated based on the data from the corresponding subgroup.

'Using data twice', first in order to obtain the prior mixture distributions informing the posterior and based on the distributions of the two defining categories each article is a member of, namely its field of research at a given disciplinary level (disciplinary group, discipline or subdiscipline) and the best quartile performance of the journal it was published in.

When data consist of a relatively small number of observations, the posterior distributions are more sensitive to the prior distributions, which allows to derive credible inferences at all disciplinary levels and for all journal quartile categories on the basis of reliable
information.

### Prior mixture elicitation

Taking advantage of the hierarchical and multivariate structure of the
PRINT dataset, the prior mixture of each article subgroup corresponds to
what values can reasonably be expected given what is known about the
different parent categories that defines it and it is a subset of. e
larger-scale distributions of the different parent categories the
subgroup belongs to. For example, it is expected that the proportion of
Biology Q1 articles belonging to the parametric anova type subgroup is
equally determined by the proportion of Biology articles containing
parametric ANOVA expressions and the proportion of Q1 articles belonging
to the parametric subgroup.

To estimate the proportion of statistically transparent papers for all
disciplinary groups, disciplines, and subdisciplines, grouped by journal
best quartile, a top-down bayesian proportion estimation procedure was
designed and implemented using various R packages. Summarily, at each
disciplinary level, statistical transparency proportions for the
different statistical assumption tests will be estimated using a
binomial likelihood function and beta distribution priors build from
higher-level data.


In the case of ratio estimations, to estimate the proportion of articles containing expressions referring to each of the different ANOVA types., beta distributions were generated for each quartile and field (disciplinary group, discipline or subdiscipline). with successes referring to the number of articles including expressions and failures including all articles that to do not include any.

More specifically, for each subgroup within the classification scheme adopted in the current project, beta-binomial conjugate pairs were used to estimate the proportion of statistically transparent ANOVA-related articles, while beta-negative binomial conjugate pairs were designed to
estimate citation counts.

For both proportion and count estimations, beta mixture priors were generated using a fit-for-purpose parameter estimation algorithm based on quartile and field information.

Given that the dataset contains only a portion of all articles published
in 2015 and indexed in the Web of Science and in the absence of any
information regarding the information regarding both statistical
transparency ratios and mean citation counts, it is reasonable to assume
that each results , beta distributions included in each prior mixture
will be weighted as such: one beta distribution weighted in proportion
to the ratio of WoS-indexed papers harvested for the corresponding
discipline level (disciplinary group, discipline or subdiscpline) and
best journal quartile (Q1, Q2, Q3 or Q4); the remaining weights are
equally split amongst three different beta distributions, namely 1) the
above-mentioned beta distribution for the quartile-field subgroup, 2)
the beta distribution for the corresponding field (irrespective of
quartile rank), and the beta distribution for the corresponding quartile
for all sibling fields (having the same parent)

This procedure ensures that estimations will not only reflect tendancies
found among journals of the same quartiles, but also take into account
the cultural specifics of scientific fields at the different levels of
granularity.

Parameters for the priors are estimated using the method of moments
approach based on the unbiased estimator of variance.

The choice of a beta distribution-based prior is easy to justify. First,
it is defined on the interval $[0,1]$, which is the same interval the
different ratios we aim to estimate are defined over. It is also very
flexible, as its two shape parameters can easily be configured using
successes and failures at the different levels of our dataset. But most
importantly, the beta distribution has the advantage of being a
conjugate prior for the Bernouilli distribution: a prior conjugate to a
particular likelihood is a family of prior distributions such their
coupling results in a posterior distribution which is of the same family
as the prior distribution, albeit with different, updated parameter
values. More precisely, given a set of observations, the joint use of a
Bernouilli likelihood function and a beta distribution prior entails
that the resulting posterior distribution is also a beta distribution.
More precisely Bayesian updating at the different levels of our dataset
will thus consist in a sequence of additive adjustments to the Beta
distribution parameters.

#### Proportions

To estimate the proportion of articles belonging to each anova type
group for all disciplinary and quartile subgroups, beta-binomial
distributions were generated based on the number of dataset articles
that belong to a given anova type (successes) compared on the number of
articles from the same disciplinary/quartile subgroup that do not
(failures).

Prior distributions for each disciplinary/quartile subgroup are generated by creating a mixture of evenly-weighted beta distributions based on the success/failure ratios of all articles included in the corresponding field (disciplinary group, discipline or subdisc, depending on the level of analysis) and best_quartile. For example, the prior distribution mixture for the subgroup including articles published in Q1 Biology journals and belonging to the parametric ANOVA subgroup will contain two different beta distributions: one based on the number of Biology articles that belong to the parametric anova subgroup compared to the number of Biology articles that do not, and another distribution expressing the proportion of Q1 articles that belong to the parametric anova subgroup. Success/failures ratios of both distributions are then adjusted based on the relative frequency of articles belonging to the article subgroup analysed. Using the above-mentioned example, the 15132 Biology Q1 articles belonging the parametric anova type subgroup represents $(15132/20524) * 100 = 73.73%$ of all Biology articles belonging to the parametric anova type subgroup and $(15132/90193) * 100 = 16.78$ of all Q1 articles belonging to the parametric anova type subgroup; in order to adjust the effect of both distributions to the size of the article subgroup analysed, the number of successes and failures for all Biology and Q1 articles belonging to the parametric anova type subgroup are adjusted using these ratios, which results in success/failure counts that have the same total frequency as the Biology Q1 parametric subgroup; the results ratios for the two prior distributions can thus be considered as expected success/failures ratios for Biology Q1 parametric papers, given what is know about success/failures ratios for Biology and Q1 papers
respectively.

Following these procedures, posterior distributions are then obtained by updating the prior mixture based on the success/failure ratio observed for the article subgroup analysed, and a single posterior distribution is generated by randomly sampling values from the posterior mixture based on the updated weights assigned to the beta distributions it contains. Finally, *maximum a posteriori* and equal-tailed 95% credible interval estimates of all posterior distributions are obtained using the bayestestR package.

As regards to priors, in the case disciplinary group estimations as well as for estimations at the subsequent lower levels, beta distributions parameterized using proportions obtained at the upper level will be used. More precisely, for all subgroups, beta distribution priors will be generated by using the success/failure ratio found in the upper-level group of the corresponding quartile, multiplied by the article frequency of the subgroup

In the case of the disciplinary group proportions, estimations were made using a beta prior build from observations obtained from both the corresponding quartile at the article population level. For example, the proportion of anova-mentioning papers that include scedasticity-related expressions in Life Sciences journals of a given journal best quartile are estimated using a beta prior build from the (upper-level) proportion observed for all same-quartile articles found in the database, updated with the (same-level) proportion observed for all Life Sciences
articles, regardless of journal best quartile. Prior choice was here motivated by the belief that beyond disciplinary borders, interquartile differences in scientific reporting practices are similar between disciplinary groups, such that what distinguishes articles published in different quartiles of the same disciplinary group could also be expected in articles from corresponding quartiles in other disciplinary groups.

In the case of disciplinary proportions, estimations were made using a beta prior build from the proportions estimated for the corresponding quartile of the corresponding disciplinary group, updated by the proportion observed for the whole discipline, irrespective of journal quartile. For example, the proportion of anova-mentioning papers that include scedasticity-related expressions in Biology journal of a given journal best quartile will be estimated by using a beta prior build from the (higher-level) proportion estimated for Life Sciences papers of the corresponding quartile, updated with the (same-level) proportion observed for Biology papers, regardless of quartile. Results are shown in the following figure.

As for subdisciplinary proportions, estimations were made using a beta prior build from the proportions estimated for the corresponding quartile of the corresponding discipline, updated by the proportion observed for the whole subdiscipline, irrespective of journal quartile. For example, the proportion of anova-mentioning papers that include scedasticity-related expressions for a given Biology subdiscipline of any given journal best quartile will be estimated by using a beta prior build from the (higher-level) proportion estimated for biology papers of the corresponding quartile, updated with the (same-level) proportion
observed for the whole subdiscipline, regardless of quartile.

According to the \texit{Science and Engineering Indicators} published by the National Center for Science and Engineering Statistics published in October 2021, 2275565 scientific articles published in 2015 [@white2021publications].

The categories used to delineate the different fields are drawn from the journal-based classification used since the 1970s by the US National Science Foundation (NSF) in the production of its Science and Engineering Indicators, which is neither the original classification of the WoS nor that of Scopus.

#### Citation Counts

In the citation of article citations, counts for each article subgroup
are estimated using Gamma-Poisson conjugate pairs.

Priors mixtures of evenly-weighted gamma distributions for each article
subgroup are build by collecting citation counts from articles of the
corresponding best quartile, field (disciplinary group, discipline or
subdiscipline) and estimating gamma distribution parameters based on the
collected data for each group of citations. In the case of Biology Q1
papers belonging to the parametric anova type subgroup, the prior
distribution mixture is generated by regrouping citation data for
Biology papers, Q1 papers as well as papers belonging to the parametric
anova type group, and then estimating gamma parameters for each
resulting citation distribution. A single prior distribution is then
obtained by randomly sampling values from the three equiprobable
distributions included in the prior mixture, and the alpha and beta
parameters of the resulting gamma distribution are then estimated using
maximum likelihood. Finally, posterior gamma distributions for each
article subgroup are obtained by updating the $\alpha$ and $\beta$
parameters of the corresponding prior distribution based on the
conjugacy between Gamma and Poisson distributions, that is by
respectively adding the total sum of citation counts and the total
number of articles in the analysed subgroup to the $\alpha$ and $\beta$
parameters of the prior distribution.

$$ = \alpha + \sum^{n}_{i=1}x_i, \beta + n $$

Finally, maximum *a posteriori* probability and equal-tailed 95%
credible interval estimates for all posterior distributions are
retrieved once again using the bayestestR package.

```{r get_priors, echo = FALSE, message = FALSE}

load_mixture <- function(this_path, sample_ratio){
  data <- data.table::fread(here::here('data/gamma_priors', paste0(this_path, '.txt')),
                            sep = '\t')
  mix <- RBesT::mixgamma(mix = c(1, data[[1, 'shape']], data[[1, 'rate']]),
                         param = 'ab')
  if(nrow(data) > 1){
    for(i in seq(2, nrow(data))){
      new_mix <- RBesT::mixgamma(mix = c(1, data[[i, 'shape']], data[[i, 'rate']]),
                                 param = 'ab')
      mix <- RBesT::mixcombine(mix, new_mix)}
    mix[1, ] <- data[, w * sample_ratio]
  }
  return(mix)
}

get_beta_mix_parameters <- function(mix_list){
  params <- data.table::setDT(data.frame(list(w = numeric(),
                                              shape = numeric(),
                                              rate = numeric())))
  for(this_mix in mix_list){
    for(i in seq(nrow(this_mix))){
      params <- data.table::rbindlist(list(
        params, this_mix[i]), use.names = T)
    }
  }
  mix <- RBesT::mixbeta(mix = c(1, params[[1, 'shape']], params[[1, 'rate']]),
                        param = 'ab')
  if(nrow(params) > 1){
    for(i in seq(2, nrow(params))){
      new_mix <- RBesT::mixbeta(mix = c(1, params[[i, 'shape']],
                                        params[[i, 'rate']]),
                                param = 'ab')
      mix <- RBesT::mixcombine(mix, new_mix)}
  }
  mix[1, ] <- params[, w/sum(params[, w])]
  return(mix)
}

get_gamma_mix_parameters <- function(mix_list){
  params <- data.table::setDT(data.frame(list(w = numeric(),
                                              shape = numeric(),
                                              rate = numeric())))
  for(this_mix_level in mix_list){
    for(this_mix in this_mix_level){
      for(i in seq(1, ncol(this_mix))){
        these_params <- list(w = this_mix['w', i],
                             shape = this_mix['a', i],
                             rate = this_mix['b', i])
        params <- data.table::rbindlist(list(
          params, these_params), use.names = T)
      }
    }
  }
  mix <- RBesT::mixgamma(mix = c(1, params[[1, 'shape']], params[[1, 'rate']]),
                         param = 'ab')
  if(nrow(params) > 1){
    for(i in seq(2, nrow(params))){
      new_mix <- RBesT::mixgamma(mix = c(1, params[[i, 'shape']],
                                         params[[i, 'rate']]),
                                 param = 'ab')
      mix <- RBesT::mixcombine(mix, new_mix)}
  }
  mix[1, ] <- params[, w/sum(params[, w])]
  return(mix)
}

get_prior_mixture <- function(this_data, this_path, load = TRUE, iter = 20){
  if(file.exists(here::here('data/gamma_priors', paste0(this_path, '.txt'))) &
     load == TRUE &
     nrow(this_data)>=100){
    print(paste0(this_path, '.txt', ' already exists'))
    mix <- data.table::fread(here::here('data/gamma_priors',
                                        paste0(this_path, '.txt')),
                             sep = '\t')
    return(mix)
  } else{
    print(paste('Getting mixture for', this_path))
    mix <- RBesT::automixfit(this_data[, incremented_citations],
                             type = 'gamma', thresh = 0,
                             Nc = seq(1, iter))
    mix <- data.table::setDT(
      as.data.frame(list(w = mix[1, ], shape = mix[2, ], rate = mix[3, ])))
    data.table::fwrite(x = mix,
                       file =  here::here('data/gamma_priors',
                                          paste0(this_path, '.txt')),
                       sep = '\t')
  }
}

get_prior_mixture2 <- function(this_data, this_path, load = TRUE){
  if(file.exists(here::here('data/gamma_priors', paste0(this_path, '.txt'))) & load == TRUE){
    print(paste0(this_path, '.txt', ' already exists'))
    mix <- data.table::fread(here::here('data/gamma_priors', paste0(this_path, '.txt')),
                             sep = '\t')
    return(mix)
  } else{
    print(paste('Getting mixture2 for', this_path))
    base_aic <- 0
    this_data <- this_data[, incremented_citations]
    tryCatch(
      for(i in seq(length(this_data))){
        i<<-i
        mix <- mixR::mixfit(this_data, ncomp = i, family = 'gamma')
        if(mix$aic > base_aic){
          base_aic <- mix$aic
          mix <- data.table::setDT(
            as.data.frame(list(w = mix$pi,
                               shape = mix$alpha,
                               rate = mix$lambda)))
          data.table::fwrite(
            x = mix,
            file =  here::here('data/gamma_priors', paste0(this_path, '.txt')),
            sep = '\t')
        } else {
          print('New mix has lower AIC')
          remove(i)
          break
        }
      },
      error = function(err){print(paste0('Unable to continue: ', err))},
      finally = function(err){remove(i)}
    )
  }
}

get_prior_mixture3 <- function(this_data, this_path, load = TRUE){
  if(file.exists(here::here('data/gamma_priors', paste0(this_path, '.txt'))) & load == TRUE){
    print(paste0(this_path, '.txt', ' already exists'))
    mix <- data.table::fread(here::here('data/gamma_priors', paste0(this_path, '.txt')),
                             sep = '\t')
    return(mix)
  } else{
    print(paste('Getting mixture3 for', this_path))
    base_loglik <- -1000000000
    this_data <- this_data[, incremented_citations]
    tryCatch(
      for(i in seq(length(this_data))){
        i<<-i
        mix <- mixtools::gammamixEM(this_data, k = i, maxit = 50000, maxrestarts = 4)
        if(mix$loglik > base_loglik){
          base_loglik <- mix$loglik
          mix <-
            mix <- data.table::setDT(
              as.data.frame(list(w = mix$lambda,
                                 shape = mix$gamma.pars['alpha',],
                                 rate = 1/mix$gamma.pars['beta',])))
          
          data.table::fwrite(
            x = mix,
            file =  here::here('data/gamma_priors', paste0(this_path, '.txt')),
            sep = '\t')
        } else {
          print('New mix has lower LogLik')
          remove(i)
          break
        }
      }, error = function(err){
        print('Unable to continue')
        remove(i)}
    )
  }
}

try_all_mixtures <- function(this_data, this_path, load_this = TRUE){
  if(!file.exists(here::here('data/gamma_priors', paste0(this_path, '.txt'))) | load_this == F){
    try(get_prior_mixture(this_data, this_path, load = load_this))}
  if(!file.exists(here::here('data/gamma_priors', paste0(this_path, '.txt'))) | load_this == F){
    print('First algorithm failed. Trying second algorithm')
    try(get_prior_mixture2(this_data,this_path, load = load_this))
  }
  if(!file.exists(here::here('data/gamma_priors', paste0(this_path, '.txt'))) | load_this == F){
    print('Second algorithm failed. Trying third algorithm')
    try(get_prior_mixture3(this_data,this_path, load = load_this))
  }
}

get_level_priors <- function(this_data, this_level, with_all_attributes = TRUE){
  print(paste('Getting all level priors for', this_level))
  try_all_mixtures(this_data, this_level)
  for (this_quartile in this_data[, unique(best_quartile)]){
    try_all_mixtures(this_data[best_quartile == this_quartile],
                     paste0(this_level, '_', this_quartile))
    for(this_type in this_data[, unique(anova_type)]){
      try_all_mixtures(this_data[anova_type == this_type],
                       ifelse(this_type %in% c('none', 'both'),
                              paste0(this_level, '_', this_type, '_types'),
                              paste0(this_level, '_', this_type)))
      try_all_mixtures(this_data[anova_type == this_type &
                                   best_quartile == this_quartile],
                       ifelse(this_type %in% c('none', 'both'),
                              paste0(this_level, '_',
                                     this_quartile, '_',
                                     this_type, '_types'),
                              paste0(this_level, '_',
                                     this_quartile, '_', this_type)))
      for(this_test in this_data[, unique(assumption_test)]){
        try_all_mixtures(this_data[assumption_test == this_test],
                         ifelse(this_test %in% c('none', 'both'),
                                paste0(this_level, '_', this_test, '_tests'),
                                paste0(this_level, '_', this_test)))
        try_all_mixtures(this_data[best_quartile == this_quartile &
                                     assumption_test == this_test],
                         ifelse(this_test %in% c('none', 'both'),
                                paste0(this_level, '_',
                                       this_quartile, '_',
                                       this_test, '_tests'),
                                paste0(this_level, '_',
                                       this_quartile, '_',
                                       this_test)))
        try_all_mixtures(this_data[anova_type == this_type &
                                     assumption_test == this_test],
                         ifelse(this_type %in% c('none', 'both'),
                                ifelse(this_test %in% c('none', 'both'),
                                       paste0(this_level, '_', this_type, '_types_',
                                              this_test, '_tests'),
                                       paste0(this_level, '_', this_type, '_types_',
                                              this_test)),
                                ifelse(this_test %in% c('none', 'both'),
                                       paste0(this_level, '_', this_type, '_',
                                              this_test, '_tests'),
                                       paste0(this_level, '_', this_type, '_',
                                              this_test))))
        if(with_all_attributes == TRUE){
          try_all_mixtures(this_data[best_quartile == this_quartile &
                                       anova_type == this_type &
                                       assumption_test == this_test],
                           ifelse(this_type %in% c('none', 'both'),
                                  ifelse(this_test %in% c('none', 'both'),
                                         paste0(this_level, '_', this_quartile, '_',
                                                this_type, '_types_', this_test, '_tests'),
                                         paste0(this_level, '_', this_quartile, '_',
                                                this_type, '_types_', this_test)),
                                  ifelse(this_test %in% c('none', 'both'),
                                         paste0(this_level, '_', this_quartile, '_',
                                                this_type, '_', this_test, '_tests'),
                                         paste0(this_level, '_', this_quartile, '_',
                                                this_type, '_', this_test))))}
      }
    }
  }
}

get_all_priors <- function(){
  data <- get_anova_data()
  get_level_priors(data, 'all')
  for(this_disc_group in data[, unique(disc_group)]){
    disc_group_data <- data[disc_group == this_disc_group]
    get_level_priors(disc_group_data, this_disc_group)
    for(this_disc in disc_group_data[, unique(disc)]){
      disc_data <- disc_group_data[disc == this_disc]
      get_level_priors(disc_data, this_disc)
      for(this_subdisc in disc_data[, unique(subdisc)]){
        subdisc_data <- disc_data[subdisc == this_subdisc]
        get_level_priors(subdisc_data, this_subdisc, with_all_attributes = FALSE)
      }
    }
  }
}

check_file_path <- function(this_path){
  file_path <- paste0(this_path, '.txt')
  if(!file.exists(here::here('data/gamma_priors', file_path))){
    print(file_path)
  }
}
```

### Conjugate Estimations

#### ANOVA test types

##### Proportions

```{r get_anova_proportions, echo = FALSE, message = FALSE}

get_anova_beta_parameters <- function(type, sample, pop){
  successes <- pop[anova_type == type, .N]
  failures <- pop[anova_type != type, .N]
  proportion <- nrow(sample)/nrow(pop)
  return(data.table::setDT(as.data.frame(
    list(w = proportion, shape = successes, rate = failures))))
}

get_anova_level_beta_parameters <- function(type, sample, pop, with_field_quartile = TRUE){
  quartile_data <- pop[best_quartile == sample[, unique(best_quartile)]]
  params <- get_anova_beta_parameters(type, sample, pop)
  if(with_field_quartile){
    params <- data.table::rbindlist(list(
      params, get_anova_beta_parameters(type, sample, quartile_data)))
  }
  return(params)
}

get_anova_beta_prior <- function(type, level, sample, pop){
  mixlist <- list(get_anova_level_beta_parameters(type, sample, pop))
  disc_group_data <- pop[disc_group == sample[, unique(disc_group)]]
  if(level == 'disc_group'){
    mixlist <- rlist::list.append(
      mixlist,
      get_anova_level_beta_parameters(type, sample, disc_group_data,
                                      with_field_quartile = F))
  }
  else{
    disc_data <- pop[disc == sample[, unique(disc)]]
    mixlist <- rlist::list.append(
      mixlist,
      get_anova_level_beta_parameters(type, sample, disc_group_data))
    if(level == 'disc'){
      mixlist <- rlist::list.append(
        mixlist,
        get_anova_level_beta_parameters(type, sample, disc_data,
                                        with_field_quartile = F))
    }
    else{
      subdisc_data <- pop[subdisc == sample[, unique(subdisc)]]
      mixlist <- rlist::list.append(
        mixlist,
        get_anova_level_beta_parameters(type, sample, disc_data),
        get_anova_level_beta_parameters(type, sample, subdisc_data,
                                        with_field_quartile = F))
    }
  }
  return(get_beta_mix_parameters(mixlist))
}

get_anova_proportion <- function(type, level, field, sample, freq, pop){
  prior <- get_anova_beta_prior(type, level, sample, pop)
  post <-
    RBesT::postmix(priormix = prior,
                   data = sample[, as.numeric(anova_type == type)])
  post_dist <- RBesT::rmix(mix = post, n = freq)
  post_dist_map <- (bayestestR::map_estimate(post_dist)[1])
  post_dist_ci <- bayestestR::bci(post_dist, ci = 0.95)
  proportion_data <- list(
    level = level,
    field = field,
    disc_group = sample[, unique(disc_group)],
    disc = ifelse(length(sample[, unique(disc)])==1,
                  sample[, unique(disc)], NA),
    subdisc = ifelse(length(sample[, unique(subdisc)])==1,
                     sample[, unique(subdisc)], NA),
    best_quartile = sample[, unique(best_quartile)],
    anova_type = type,
    prop_map = post_dist_map * 100,
    prop_low = (post_dist_ci$CI_low) *100,
    prop_high = (post_dist_ci$CI_high) * 100,
    expected_frequency = post_dist_map * freq)
  return(proportion_data)
}

create_results_data_table <- function(stat_type, with_tests = FALSE){
  data <- data.table::data.table(
      level = character(),
      field = character(),
      disc_group = character(),
      disc = character(),
      subdisc = character(),
      anova_type = character(),
      prop_map = numeric(),
      prop_low = numeric(),
      prop_high = numeric(),
      expected_frequency = numeric()
    )
  if(stat_type == 'proportion'){
    proportion_data$prop_map = numeric()
    proportion_data$prop_low = numeric()
    proportion_data$prop_high = numeric()}
  else{
    proportion_data$diff_map = numeric()
    proportion_data$diff_low = numeric()
    proportion_data$diff_high = numeric()
  }
  if(with_tests){proportion_data$assumption_test = character()}
  return(data)
}

get_all_anova_proportions <- function(){
  proportion_data <-
    create_results_data_table(stat_type = 'proportion')
  frequencies <- get_article_frequencies()
  data <- get_anova_data()
  for(this_type in data[anova_type != 'none', unique(anova_type)]){
    for(this_quartile in data[, unique(best_quartile)]){
      for(this_disc_group in data[anova_type == this_type, unique(disc_group)]){
        disc_group_data <- data[disc_group == this_disc_group]
        sample <- disc_group_data[best_quartile == this_quartile]
        if(sample[, any(anova_type != this_type)]){
          freq <- frequencies[best_quartile == this_quartile &
                                field == this_disc_group,
                              wos_articles]
          proportion_data <- data.table::rbindlist(list(
            proportion_data,
            get_anova_proportion(this_type, 'disc_group', this_disc_group,
                                 sample, freq, data)),
            use.names = T,
            fill = T)}
        for(this_disc in sample[anova_type == this_type, unique(disc)]){
          print(paste0('Computing proportions for ', this_quartile, ' ',
                       this_disc, ' ', this_type))
          disc_data <- data[disc == this_disc]
          sample <- disc_data[best_quartile == this_quartile]
          if(sample[, any(anova_type != this_type)]){
            freq <- frequencies[best_quartile == this_quartile &
                            field == this_disc,
                          wos_articles]
            proportion_data <- data.table::rbindlist(list(
              proportion_data,
              get_anova_proportion(this_type, 'disc', this_disc,
                                   sample, freq, data)),
              use.names = T,
              fill = T)}
          for(this_subdisc in sample[anova_type == this_type, unique(subdisc)]){
            subdisc_data <- data[subdisc == this_subdisc]
            sample <- subdisc_data[best_quartile == this_quartile]
            if(sample[, any(anova_type != this_type)]){
              freq <- frequencies[best_quartile == this_quartile &
                                    field == this_subdisc,
                                  wos_articles]
              proportion_data <- data.table::rbindlist(list(
                proportion_data,
                get_anova_proportion(this_type, 'subdisc', this_subdisc,
                                     sample, freq, data)),
                use.names = T,
                fill = T)}
          }
        }
      }
    }
  }
  data.table::fwrite(x = proportion_data,
                     file = here::here('data/estimates',
                                       'anova_proportions.txt'),
                     sep = '\t')
}
```

##### Citation Counts

```{r get_anova_count_diffs, echo = FALSE, message = FALSE}

load_anova_count_diffs <- function(){
  return(data.table::fread(here::here('data/estimates',
                                      'anova_count_diffs.txt'),
                           sep = '\t'))}

get_anova_level_gamma_parameters <- function(field, sample, pop,
                                             with_field_quartile_type = TRUE){
  this_quartile <- sample[, unique(best_quartile)]
  this_type <- sample[, unique(anova_type)]
  mix_list <- list(load_mixture(field, nrow(sample)/nrow(pop)))
  mix_list <- rlist::list.append(
    mix_list,
    load_mixture(paste0(field, '_', this_quartile),
                 nrow(sample)/nrow(pop[best_quartile == this_quartile])),
    load_mixture(ifelse(this_type %in% c('none', 'both'),
                        paste0(field, '_', this_type, '_types'),
                        paste0(field, '_', this_type)),
                 nrow(sample)/nrow(pop[anova_type == this_type])))
  if(with_field_quartile_type){
    mix_list <- rlist::list.append(
      mix_list,
      load_mixture(ifelse(this_type %in% c('none', 'both'),
                          paste0(field, '_', this_quartile, '_', this_type, '_types'),
                          paste0(field, '_', this_quartile, '_', this_type)),
                   nrow(sample)/nrow(pop[best_quartile == this_quartile &
                                           anova_type == this_type])))}
  return(mix_list)
}

get_anova_gamma_prior <- function(level, sample, pop){
  this_type <- sample[, unique(anova_type)]
  this_disc_group <- sample[, unique(disc_group)]
  mixlist <- list(get_anova_level_gamma_parameters('all', sample, pop))
  disc_group_data <- pop[disc_group == this_disc_group]
  if(level == 'disc_group'){
    mixlist <- rlist::list.append(
      mixlist,
      get_anova_level_gamma_parameters(this_disc_group,
                                       sample, disc_group_data,
                                       with_field_quartile_type = F))
  }
  else{
    this_disc <- sample[, unique(disc)]
    disc_data <- pop[disc == this_disc]
    mixlist <- rlist::list.append(
      mixlist,
      get_anova_level_gamma_parameters(this_disc_group, sample,
                                       disc_group_data))
    if(level == 'disc'){
      mixlist <- rlist::list.append(
        mixlist,
        get_anova_level_gamma_parameters(this_disc, sample, disc_data,
                                         with_field_quartile_type = F))
    }
    else{
      this_subdisc <- sample[, unique(subdisc)]
      subdisc_data <- pop[subdisc == this_subdisc]
      mixlist <- rlist::list.append(
        mixlist,
        get_anova_level_gamma_parameters(this_disc, sample, disc_data),
        get_anova_level_gamma_parameters(this_subdisc, sample, subdisc_data,
                                         with_field_quartile_type = F))
    }
  }
  return(get_gamma_mix_parameters(mixlist))
}

get_anova_count_diffs <- function(level, field, type, sample, pop, freq){
  type_sample <- sample[anova_type == type]
  none_sample <- sample[anova_type == 'none']
  type_prior <- get_anova_gamma_prior(level, type_sample, pop)
  none_prior <- get_anova_gamma_prior(level, none_sample, pop)
  type_post <- RBesT::postmix(
    priormix = type_prior,
    data = type_sample[, incremented_citations])
  none_post <- RBesT::postmix(
    priormix = none_prior,
    data = none_sample[, incremented_citations])
  type_dist <- sample(RBesT::rmix(mix = type_post, n = freq))
  none_dist <- sample(RBesT::rmix(mix = none_post, n = freq))
  diff_dist <- ((type_dist - none_dist)/(none_dist - 1)) * 100
  diff_dist_ci <- bayestestR::bci(diff_dist, ci = 0.95)
  this_diff <- list(
    level = level,
    field = field,
    disc_group = type_sample[, unique(disc_group)],
    disc = ifelse(length(type_sample[, unique(disc)]) == 1,
                  type_sample[, unique(disc)], NA),
    subdisc = ifelse(length(type_sample[, unique(subdisc)]) == 1,
                     type_sample[, unique(subdisc)], NA),
    best_quartile = type_sample[, unique(best_quartile)],
    anova_type = type,
    diff_map = bayestestR::map_estimate(diff_dist)[1],
    diff_low = diff_dist_ci$CI_low,
    diff_high = diff_dist_ci$CI_high
  )
  return(this_diff)
}

get_all_anova_count_diffs <- function(){
  diff_data <- create_results_data_table(stat_type = 'diff')
  data <- get_anova_data()
  prop <- load_anova_proportions()
  for(this_type in prop[, unique(anova_type)]){
    type_prop <- prop[anova_type == this_type]
    for(this_quartile in type_prop[, unique(best_quartile)]){
      quartile_prop <- type_prop[best_quartile == this_quartile]
      for(this_disc_group in quartile_prop[level == 'disc_group', unique(field)]){
        disc_group_prop <- quartile_prop[disc_group == this_disc_group]
        sample <- data[disc_group == this_disc_group &
                         best_quartile == this_quartile]
        freq <- disc_group_prop[field == this_disc_group,
                                floor(expected_frequency)]
        if(nrow(sample[anova_type == 'none'])>2 &
           nrow(sample[anova_type == this_type])>2 &
           freq > 2){
          diff_data <- data.table::rbindlist(list(
            diff_data,
            get_anova_count_diffs('disc_group', this_disc_group, this_type,
                                  sample, data, freq)),
            use.names = T,
            fill = T)}
        for(this_disc in disc_group_prop[level == 'disc', unique(field)]){
          print(paste('Computing count differences for ', this_disc, this_quartile, this_type))
          sample <- data[disc == this_disc & best_quartile == this_quartile]
          disc_prop <- disc_group_prop[disc == this_disc]
          freq <- disc_prop[field == this_disc,
                            floor(expected_frequency)]
          if(nrow(sample[anova_type == 'none'])>2 &
             nrow(sample[anova_type == this_type])>2 &
             freq > 2){
            diff_data <- data.table::rbindlist(list(
              diff_data,
              get_anova_count_diffs('disc', this_disc, this_type,
                                    sample, data, freq)),
              use.names = T,
              fill = T)
          }
          for(this_subdisc in disc_prop[level == 'subdisc', unique(field)]){
            print(paste(this_subdisc, this_quartile, this_type))
            sample <- data[subdisc == this_subdisc & best_quartile == this_quartile]
            subdisc_prop <- disc_prop[subdisc == this_subdisc]
            freq <- subdisc_prop[field == this_subdisc,
                                 floor(expected_frequency)]
            if(nrow(sample[anova_type == 'none'])>2 &
               nrow(sample[anova_type == this_type])>2 &
               freq > 2){
              diff_data <- data.table::rbindlist(list(
                diff_data,
                get_anova_count_diffs('subdisc', this_subdisc, this_type,
                                      sample, data, freq)),
                use.names = T,
                fill = T)
            }
          }
        }
      }
    }
  }
  data.table::fwrite(x = diff_data,
                     file = here::here('data/estimates',
                                       'anova_count_diffs.txt'),
                     sep = '\t')
  return(diff_data)
}

```

#### Assumption Tests

##### Proportions

```{r get_assumption_proportions, echo = FALSE, message = FALSE}

load_anova_proportions <- function(){
  return(data.table::fread(here::here('data/estimates',
                                      'anova_proportions.txt')))
}

get_assumption_beta_parameters <- function(this_test, sample, population){
  successes <- population[assumption_test == this_test, .N]
  failures <- population[assumption_test != this_test, .N]
  proportion <- nrow(sample)/nrow(population)
  return(data.table::setDT(as.data.frame(list(w = proportion,
                                              shape = successes,
                                              rate = failures))))
}

get_assumption_level_beta_parameters <- function(this_test, sample, pop,
                                                 with_quartile_type_test = TRUE){
  this_quartile <- sample[, unique(best_quartile)]
  this_type <- sample[, unique(anova_type)]
  params <- data.table::rbindlist(list(
    get_assumption_beta_parameters(this_test, sample, pop),
    get_assumption_beta_parameters(this_test, sample,
                                   pop[best_quartile == this_quartile]),
    get_assumption_beta_parameters(this_test, sample,
                                   pop[anova_type == this_type]),
    get_assumption_beta_parameters(this_test, sample,
                                   pop[assumption_test == this_test]),
    get_assumption_beta_parameters(this_test, sample,
                                   pop[best_quartile == this_quartile &
                                         anova_type == this_type]),
    get_assumption_beta_parameters(this_test, sample,
                                   pop[best_quartile == this_quartile &
                                         assumption_test == this_test]),
    get_assumption_beta_parameters(this_test, sample,
                                   pop[assumption_test == this_test &
                                         anova_type == this_type])))
    if(with_quartile_type_test){
      data.table::rbindlist(list(
        params,
        get_assumption_beta_parameters(this_test, sample,
                                       pop[best_quartile == this_quartile &
                                             anova_type == this_type &
                                             assumption_test == this_test])))}
  return(params)
}

get_assumption_beta_prior <- function(this_test, this_level, sample_data, this_data){
  mixlist <- list(get_assumption_level_beta_parameters(this_test, sample_data, this_data))
  this_quartile <- sample_data[, unique(best_quartile)]
  this_type <- sample_data[, unique(anova_type)]
  disc_group_data <- this_data[disc_group == sample_data[, unique(disc_group)]]
  if(this_level == 'disc_group'){
    mixlist <- rlist::list.append(
      mixlist,
      get_assumption_level_beta_parameters(this_test, sample_data, disc_group_data,
                                          with_quartile_type_test = F))
  }
  else{
    disc_data <- this_data[disc == sample_data[, unique(disc)]]
    mixlist <- rlist::list.append(
      mixlist,
      get_assumption_level_beta_parameters(this_test, sample_data, disc_group_data))
    if(this_level == 'disc'){
      mixlist <- rlist::list.append(
        mixlist,
        get_assumption_level_beta_parameters(this_test, sample_data, disc_data,
                                             with_quartile_type_test = F))
    }
    else{
      subdisc_data <- this_data[subdisc == sample_data[, unique(subdisc)]]
      mixlist <- rlist::list.append(
        mixlist,
        get_assumption_level_beta_parameters(this_test, sample_data, disc_data),
        get_assumption_level_beta_parameters(this_test, sample_data, subdisc_data,
                                             with_quartile_type_test = F))
    }
  }
  return(get_beta_mix_parameters(mixlist))
}

get_assumption_proportion <- function(test, level, field,
                                      sample, freq, pop){
  prior <- get_assumption_beta_prior(test, level, sample, pop)
  post <- RBesT::postmix(priormix = prior,
                         data = sample[, as.numeric(assumption_test == test)])
  dist <- RBesT::rmix(mix = post, n = freq)
  dist_map <- bayestestR::map_estimate(dist)[1]
  dist_ci <- bayestestR::bci(dist, ci = 0.95)
  prop_data <- list(
    level = level,
    field = field,
    disc_group = sample[, unique(disc_group)],
    disc = ifelse(length(sample[, unique(disc)])==1,
                  sample[, unique(disc)], NA),
    subdisc = ifelse(length(sample[, unique(subdisc)])==1,
                     sample[, unique(subdisc)], NA),
    best_quartile = sample[, unique(best_quartile)],
    anova_type = sample[, unique(anova_type)],
    assumption_test = test,
    prop_map = dist_map * 100,
    prop_low = (dist_ci$CI_low) *100,
    prop_high = (dist_ci$CI_high) * 100,
    expected_frequency = dist_map * freq
  )
  return(prop_data)
}

get_all_assumption_proportions <- function(){
  proportion_data <-
    create_results_data_table(stat_type == 'proportion', with_tests = TRUE)
  data <- get_anova_data() %>%
    .[anova_type != 'none']
  prop <- load_anova_proportions()
  for(this_test in data[assumption_test != 'none', unique(assumption_test)]){
    for(this_type in data[, unique(anova_type)]){
      type_prop <- prop[anova_type == this_type]
      for(this_quartile in type_prop[, unique(best_quartile)]){
        q_prop <- type_prop[best_quartile == this_quartile]
        for(this_disc_group in q_prop[level == 'disc_group', unique(field)]){
          disc_group_prop <- q_prop[disc_group == this_disc_group]
          sample <- data[disc_group == this_disc_group &
                           best_quartile == this_quartile &
                           anova_type == this_type]
          freq <- disc_group_prop[field == this_disc_group,
                                  floor(expected_frequency)]
          if(sample[, any(assumption_test != this_test)] &
             nrow(sample[assumption_test == this_test])>2 &
             freq > 2){
            proportion_data <- data.table::rbindlist(list(
              proportion_data,
              get_assumption_proportion(this_test, 'disc_group', this_disc_group,
                                        sample, freq, data)),
              use.names = T,
              fill = T)}
          for(this_disc in disc_group_prop[level == 'disc', unique(field)]){
            disc_prop <- disc_group_prop[disc == this_disc]
            disc_data <- data[disc == this_disc]
            sample <- disc_data[best_quartile == this_quartile &
                                  anova_type == this_type]
            freq <- disc_prop[field == this_disc,
                              floor(expected_frequency)]
            if(sample[, any(assumption_test != this_test)] &
               nrow(sample[assumption_test == this_test])>2 &
               freq > 2){
              print(paste0('Computing proportions for ', this_quartile, ' ',
                           this_disc, ' ', this_type, ' ', this_test))
              proportion_data <- data.table::rbindlist(list(
                proportion_data,
                get_assumption_proportion(this_test, 'disc', this_disc,
                                          sample, freq, data)),
                use.names = T,
                fill = T)}
            for(this_subdisc in disc_prop[level == 'subdisc', unique(field)]){
              subdisc_prop <- disc_prop[subdisc == this_subdisc]
              subdisc_data <- data[subdisc == this_subdisc]
              sample <- subdisc_data[best_quartile == this_quartile &
                                       anova_type == this_type]
              freq <- subdisc_prop[field == this_subdisc,
                                   floor(expected_frequency)]
              if(sample[, any(assumption_test != this_test)] &
                 nrow(sample[assumption_test == this_test])>2 &
                 freq > 2){
                print(paste0('Processing ', this_quartile, ' ',
                             this_subdisc, ' ', this_type, ' ', this_test))
                proportion_data <- data.table::rbindlist(list(
                  proportion_data,
                  get_assumption_proportion(this_test, 'subdisc', this_subdisc,
                                            sample, freq, data)),
                  use.names = T,
                  fill = T)}
            }
          }
        }
      }
    }
  }
  data.table::fwrite(x = proportion_data,
                     file = here::here('data/estimates',
                                       'assumption_proportionss.txt'),
                     sep = '\t')
  return(proportion_data)
}

```


#### Assumption Count Differences

```{r get_assumption_count_diffs, echo = FALSE, message = FALSE}

load_assumption_proportions <- function(){
  return(data.table::fread(here::here('data/estimates',
                                      'assumption_proportions.txt')))
}

load_assumption_count_diffs <- function(){
  return(data.table::fread(here::here('data/estimates',
                                      'assumption_count_diffs.txt')))
}

get_assumption_level_gamma_parameters <- function(field, sample, pop,
                                                  with_field_quartile_type_test = TRUE){
  this_quartile <- sample[, unique(best_quartile)]
  this_type <- sample[, unique(anova_type)]
  this_test <- sample[, unique(assumption_test)]
  mix_list <- list(load_mixture(field, nrow(sample)/nrow(pop)))
  mix_list <- rlist::list.append(
    mix_list,
    load_mixture(paste0(field, '_', this_quartile),
                 nrow(sample)/nrow(pop[best_quartile == this_quartile])),
    load_mixture(ifelse(this_type %in% c('none', 'both'),
                        paste0(field, '_', this_type, '_types'),
                        paste0(field, '_', this_type)),
                 nrow(sample)/nrow(pop[anova_type == this_type])),
    load_mixture(ifelse(this_test %in% c('none', 'both'),
                        paste0(field, '_', this_test, '_tests'),
                        paste0(field, '_', this_test)),
                 nrow(sample)/nrow(pop[assumption_test == this_test])),
    load_mixture(ifelse(this_type %in% c('none', 'both'),
                        paste0(field, '_', this_quartile, '_', this_type, '_types'),
                        paste0(field, '_', this_quartile, '_', this_type)),
                 nrow(sample)/nrow(pop[best_quartile == this_quartile &
                                         anova_type == this_type])),
    load_mixture(ifelse(this_test %in% c('none', 'both'),
                        paste0(field, '_', this_quartile, '_', this_test, '_tests'),
                        paste0(field, '_', this_quartile, '_', this_test)),
                 nrow(sample)/nrow(pop[best_quartile == this_quartile &
                                         assumption_test == this_test])),
    load_mixture(ifelse(this_type %in% c('none', 'both'),
                        ifelse(this_test %in% c('none', 'both'),
                               paste0(field, '_', this_type, '_types_',
                                      this_test, '_tests'),
                               paste0(field, '_', this_type, '_types_',
                                      this_test)),
                        ifelse(this_test %in% c('none', 'both'),
                               paste0(field, '_', this_type, '_',
                                      this_test, '_tests'),
                               paste0(field, '_', this_type, '_',
                                      this_test))),
                 nrow(sample)/nrow(pop[anova_type == this_type &
                                         assumption_test == this_test])))
  if(with_field_quartile_type_test){
    mix_list <- rlist::list.append(
      mix_list,
      load_mixture(ifelse(this_type %in% c('none', 'both'),
                          ifelse(this_test %in% c('none', 'both'),
                                 paste0(field, '_', this_quartile, '_',
                                        this_type, '_types_', this_test, '_tests'),
                                 paste0(field, '_', this_quartile, '_',
                                        this_type, '_types_', this_test)),
                          ifelse(this_test %in% c('none', 'both'),
                                 paste0(field, '_', this_quartile, '_',
                                        this_type, '_', this_test, '_tests'),
                                 paste0(field, '_', this_quartile, '_',
                                        this_type, '_', this_test))),
                   nrow(sample)/nrow(pop[anova_type == this_type &
                                           assumption_test == this_test &
                                           best_quartile == this_quartile])))}
  return(mix_list)
}

get_assumption_gamma_prior <- function(this_level, sample, data){
  this_type <- sample[, unique(anova_type)]
  this_test <- sample[, unique(assumption_test)]
  this_disc_group <- sample[, unique(disc_group)]
  mixlist <- list(get_assumption_level_gamma_parameters('all', sample, data))
  disc_group_data <- data[disc_group == this_disc_group]
  if(this_level == 'disc_group'){
    mixlist <- rlist::list.append(
      mixlist,
      get_assumption_level_gamma_parameters(this_disc_group,
                                            sample, disc_group_data,
                                            with_field_quartile_type_test = F))
  }
  else{
    this_disc <- sample[, unique(disc)]
    disc_data <- data[disc == this_disc]
    mixlist <- rlist::list.append(
      mixlist,
      get_assumption_level_gamma_parameters(this_disc_group, sample,
                                            disc_group_data))
    if(this_level == 'disc'){
      mixlist <- rlist::list.append(
        mixlist,
        get_assumption_level_gamma_parameters(this_disc, sample, disc_data,
                                              with_field_quartile_type_test = F))
    }
    else{
      this_subdisc <- sample[, unique(subdisc)]
      subdisc_data <- data[subdisc == this_subdisc]
      mixlist <- rlist::list.append(
        mixlist,
        get_assumption_level_gamma_parameters(this_disc, sample, disc_data),
        get_assumption_level_gamma_parameters(this_subdisc, sample, subdisc_data,
                                              with_field_quartile_type_test = F))
    }
  }
  return(get_gamma_mix_parameters(mixlist))
}

get_assumption_count_diff <- function(this_level, this_field, this_test, sample, data, freq){
  test_sample <- sample[assumption_test == this_test]
  none_sample <- sample[assumption_test == 'none']
  test_prior <- get_assumption_gamma_prior(this_level, test_sample, data)
  none_prior <- get_assumption_gamma_prior(this_level, none_sample, data)
  test_post <- RBesT::postmix(
    priormix = test_prior,
    data = test_sample[, incremented_citations])
  none_post <- RBesT::postmix(
    priormix = none_prior,
    data = none_sample[, incremented_citations])
  test_dist <- sample(RBesT::rmix(mix = test_post, n = freq))
  none_dist <- sample(RBesT::rmix(mix = none_post, n = freq))
  diff_dist <- ((test_dist - none_dist)/(none_dist - 1)) * 100
  diff_dist_ci <- bayestestR::bci(diff_dist, ci = 0.95)
  this_diff <- list(
    level = this_level,
    field = this_field,
    disc_group = test_sample[, unique(disc_group)],
    disc = ifelse(length(test_sample[, unique(disc)]) == 1,
                  test_sample[, unique(disc)], NA),
    subdisc = ifelse(length(test_sample[, unique(subdisc)]) == 1,
                     test_sample[, unique(subdisc)], NA),
    best_quartile = test_sample[, unique(best_quartile)],
    anova_type = test_sample[, unique(anova_type)],
    assumption_test = this_test,
    diff_map = bayestestR::map_estimate(diff_dist)[1],
    diff_low = diff_dist_ci$CI_low,
    diff_high = diff_dist_ci$CI_high
  )
  return(this_diff)
}

get_all_assumption_count_diffs <- function(){
  diff_data <-
    create_results_data_table(stat_type == 'diff', with_tests = TRUE)
  data <- get_anova_data()
  prop <- load_assumption_proportions()
  for(this_type in prop[, unique(anova_type)]){
    type_prop <- prop[anova_type == this_type]
    for(this_test in data[assumption_test != 'none', unique(assumption_test)]){
      test_prop <- type_prop[assumption_test == this_test]
      for(this_quartile in data[anova_type == this_type, unique(best_quartile)]){
        q_prop <- test_prop[best_quartile == this_quartile]
        for(this_disc_group in q_prop[level == 'disc_group', unique(field)]){
          disc_group_prop <- q_prop[disc_group == this_disc_group]
          sample <- data[disc_group == this_disc_group &
                           best_quartile == this_quartile &
                           anova_type == this_type]
          freq <- disc_group_prop[field == this_disc_group,
                                  floor(expected_frequency)]
          if(nrow(sample[assumption_test == 'none'])>2 &
             nrow(sample[assumption_test == this_test])>2 &
             freq >2){
            diff_data <- data.table::rbindlist(list(
              diff_data,
              get_assumption_count_diff('disc_group', this_disc_group, this_test,
                                         sample, data, freq)),
              use.names = T,
              fill = T)
          }
          for(this_disc in disc_group_prop[level == 'disc', unique(field)]){
            print(paste(this_disc, this_quartile, this_type, this_test))
            disc_prop <- disc_group_prop[disc == this_disc]
            sample <- data[disc == this_disc &
                             best_quartile == this_quartile &
                             anova_type == this_type]
            freq <- disc_prop[field == this_disc,
                              floor(expected_frequency)]
            if(nrow(sample[assumption_test == 'none'])>2 &
               nrow(sample[assumption_test == this_test])>2 &
               freq >2){
              diff_data <- data.table::rbindlist(list(
                diff_data,
                get_assumption_count_diff('disc', this_disc, this_test,
                                           sample, data, freq)),
                use.names = T,
                fill = T)
            }
            for(this_subdisc in disc_prop[level == 'subdisc', unique(field)]){
              print(paste('Computing count differences for ',
                          this_subdisc, this_quartile, this_type, this_test))
              subdisc_prop <- disc_prop[subdisc == this_subdisc]
              sample <- data[subdisc == this_subdisc &
                               best_quartile == this_quartile &
                               anova_type == this_type]
              freq <- subdisc_prop[field == this_subdisc, floor(expected_frequency)]
              if(nrow(sample[assumption_test == 'none'])>2 &
                 nrow(sample[assumption_test == this_test])>2 &
                 freq >2){
                diff_data <- data.table::rbindlist(list(
                  diff_data,
                  get_assumption_count_diff('subdisc', this_subdisc, this_test,
                                             sample, data, freq)),
                  use.names = T,
                  fill = T)}
            }
          }
        }
      }
    }
  }
  data.table::fwrite(x = diff_data,
                     file = here::here('data/estimates',
                                       'assumption_count_diffs.txt'),
                     sep = '\t')
  return(diff_data)
}

get_all_estimates <- function(){
  get_all_anova_proportions()
  get_all_anova_count_diffs()
  get_all_assumption_proportions()
  get_all_assumption_count_diffs()
}
```

# Results

## ANOVA Types

```{r plot_anova, echo = FALSE, message = FALSE}

plot_anova <- function(load = TRUE){
  if(file.exists(here::here('figures', 'plot_anova.jpg')) & load == TRUE){
    knitr::include_graphics(here::here('figures', 'plot_anova.jpg'))
  }
  else{
    data <- data.table::fread(here::here('data/estimates', 'anova_proportions.txt'),
                              sep = '\t') %>%
      .[level == 'disc_group', .(field, anova_type, best_quartile,
                                 prop_map, prop_low, prop_high, expected_frequency)] %>%
      merge(., data.table::fread(here::here('data/estimates', 'anova_count_diffs.txt'))%>%
              .[level == 'disc_group', .(field, anova_type, best_quartile,
                                         diff_map, diff_low, diff_high)],
            by = c('field', 'anova_type', 'best_quartile'),
            all.x = TRUE) %>%
      .[!is.na(diff_map)&!is.na(prop_map)] %>%
      .[, quartile := as.numeric(dplyr::recode(best_quartile,
                                               'Q1' = 1,
                                               'Q2'= 2,
                                               'Q3'= 3,
                                               'Q4'= 4))]
    this_plot <- 
      ggplot2::ggplot(data = data, aes(y = prop_map, x = diff_map,
                                       color = best_quartile,
                                       shape = anova_type)) +
      ggplot2::geom_point(size = 2) +
      scale_x_continuous(trans = scales::pseudo_log_trans())+
      ggplot2::geom_linerange(aes(ymin = prop_low, ymax = prop_high)) +
      ggplot2::geom_linerange(aes(xmin = diff_low, xmax = diff_high)) +
      #geom_linerange(position = position_dodge(width = 0.75), linewidth = 1) +
      #scale_y_discrete(limits = rev) +
      #scale_x_sqrt(breaks = scales::trans_breaks("sqrt",
                                                 #function(x) x^2, n=5)(c(0, 35))) +
      #scale_x_sqrt(breaks = c(0,1,5, 10, 20, 30),
                   #limits = c(0, 35),
                   #expand = c(0, 0)) +
      #xlab('%') +
      theme(axis.title.y = element_blank(),
            axis.title.x = element_text(face = 'bold'),
            legend.position = 'top',
            legend.direction = 'horizontal',
            legend.background = element_rect(colour = "grey50")) +
      ggh4x::facet_grid2(field~.)
    ggplot2::ggsave(filename = 'plot_anova.jpg',
                    plot = this_plot,
                    width = 12,
                    height = 8,
                    path = here::here('figures'),
                    dpi = 600)
    print(this_plot)
  }
}

plot_anova()

```

The following plot shows the percentage of anova_related articles in each discipline, grouped by anova type (x axis), disciplinary group (y axis), and SJR best quartile (color); maximum *a posteriori* values and 95% highest density intervals for each distribution are represented by points and horizontal lines respectively.

```{r plot_anova_disc_proportions, echo = FALSE, message = FALSE}
plot_anova_disc_proportions <- function(load = TRUE){
  if(file.exists(here::here('figures', 'plot_anova_disc_proportions.jpg')) & load == TRUE){
    knitr::include_graphics(here::here('figures', 'plot_anova_disc_proportions.jpg'))
  }
  else{
    data <- data.table::fread(here::here('data/estimates', 'anova_proportions.txt'),
                              sep = '\t')
    this_plot <- 
      ggplot2::ggplot(data = data[level == 'disc' #&
                                    #best_quartile != 'Q4'
                                  ],
                      aes(x = prop_map, y = field, color = best_quartile, group = best_quartile,
                          xmin = prop_low, xmax = prop_high)) +
      ggplot2::geom_point(position = position_dodge(width = 0.75), size = 2) +
      geom_linerange(position = position_dodge(width = 0.75)) +
      scale_y_discrete(limits = rev) +
      #scale_x_sqrt(breaks = scales::trans_breaks("sqrt",
                                                 #function(x) x^2, n=5)(c(0, 35))) +
      scale_x_sqrt(breaks = c(0,1,5, 10, 20, 30),
                   limits = c(0, 35),
                   expand = c(0, 0)) +
      xlab('%') +
      theme(axis.title.y = element_blank(),
            axis.title.x = element_text(face = 'bold'),
            legend.position = 'top',
            legend.direction = 'horizontal',
            legend.background = element_rect(colour = "grey50")) +
      ggh4x::facet_grid2(disc_group~forcats::fct_rev(anova_type),
                         scales = 'free_y', space = 'free_y',)
    ggplot2::ggsave(filename = 'plot_anova_disc_proportions.jpg',
                    plot = this_plot,
                    width = 12,
                    height = 8,
                    path = here::here('figures'),
                    dpi = 600)
    print(this_plot)
  }
}

plot_anova_disc_proportions()
```

```{r plot_anova_subdisc_proportions, echo = FALSE, message = FALSE}
plot_anova_subdisc_proportions <- function(load = TRUE){
  if(file.exists(here::here('figures', 'plot_anova_subdisc_proportions.jpg')) & load == TRUE){
    knitr::include_graphics(here::here('figures', 'plot_anova_subdisc_proportions.jpg'))
  }
  else{
    data <- data.table::fread(here::here('data/estimates', 'anova_proportions.txt'),
                              sep = '\t') %>%
      .[, field := stringr::str_replace_all(field, 'ogy', '.')] %>%
      .[, field := stringr::str_replace(field, 'General', 'Gen.')] %>%
      .[, field := stringr::str_replace(field, 'Clinical', 'Clin.')] %>%
      .[, field := stringr::str_replace(field, 'medical ', 'med.')] %>%
      .[, field := stringr::str_replace(field, 'Medicine', 'Med.')] %>%
      .[, field := stringr::str_replace(field, 'mental ', 'm.')] %>%
      .[, field := stringr::str_replace(field, 'Science', 'Sc.')] %>%
      .[, field := stringr::str_replace(field, 'mentary ', 'm.')] %>%
      .[, field := stringr::str_replace(field, 'pational ', 'p.')] %>%
      .[, field := stringr::str_replace(field, 'Diseases?', 'Dis.')] %>%
      .[, field := stringr::str_replace(field, 'System', 'Sys.')] %>%
      .[, field := stringr::str_replace(field, 'chemistry', 'chem.')] %>%
      .[, field := stringr::str_replace(field, 'iratory', '.')] %>%
      .[, field := stringr::str_replace(field, 'ular ', '.')] %>%
      .[, field := stringr::str_replace(field, 'ioral ', '.')]
    
    top_other_subdiscs <-
      data[level == 'subdisc' & !disc %in% c('Biology', 'Psychology')] %>%
      .[, .(max_prop = max(prop_map)),
        by = .(subdisc, disc_group)] %>%
      .[order(max_prop, decreasing = TRUE)] %>%
      .[, .(subdisc = head(subdisc, n = 5))]
    bio_plot <- 
      ggplot2::ggplot(
        data = data[disc %in% 'Biology'&
                      #best_quartile != 'Q4' &
                      anova_type == 'parametric' &
                      level == 'subdisc'],
        aes(x = prop_map, y = field, color = best_quartile, group = best_quartile,
            xmin = prop_low, xmax = prop_high)) +
      ggplot2::geom_point(position = position_dodge(width = 0.4)) +
      geom_linerange(position = position_dodge(width = 0.4)) +
      scale_y_discrete(limits = rev) +
      scale_x_sqrt(breaks = c(10, 20, 30, 40, 50),
                   limits = c(0, 60)) +
      facet_wrap(~disc, strip.position="right") +
      theme(axis.title.x = element_blank(),
            axis.title.y = element_blank())
    biomed_plot <- 
      ggplot2::ggplot(
        data = data[disc %in% 'Biomedical Research' &
                      #best_quartile != Q4 &
                      anova_type == 'parametric' &
                      level == 'subdisc'],
        aes(x = prop_map, y = field, color = best_quartile, group = best_quartile,
            xmin = prop_low, xmax = prop_high)) +
      ggplot2::geom_point(position = position_dodge(width = 0.4)) +
      geom_linerange(position = position_dodge(width = 0.4)) +
      scale_y_discrete(limits = rev) +
      scale_x_sqrt(breaks = c(10, 20, 30, 40, 50),
                   limits = c(0, 60)) +
      facet_wrap(~disc, strip.position="right") +
      theme(axis.title.x = element_blank(),
            axis.title.y = element_blank())
    med_plot <- 
      ggplot2::ggplot(
        data = data[disc %in% 'Clinical Medicine'&
                      #best_quartile != 'Q4' &
                      anova_type == 'parametric' &
                      level == 'subdisc'],
        aes(x = prop_map, y = field, color = best_quartile, group = best_quartile,
            xmin = prop_low, xmax = prop_high)) +
      ggplot2::geom_point(position = position_dodge(width = 0.4)) +
      geom_linerange(position = position_dodge(width = 0.4)) +
      scale_y_discrete(limits = rev) +
      scale_x_sqrt(breaks = c(10, 20, 30, 40, 50),
                   limits = c(0, 60)) +
      facet_wrap(~disc, strip.position="right") +
      theme(axis.title.x = element_blank(),
            axis.title.y = element_blank())
    psy_plot <- 
      ggplot2::ggplot(
        data = data[disc %in% 'Psychology'&
                      #best_quartile != 'Q4' &
                      anova_type == 'parametric' &
                      level == 'subdisc'],
        aes(x = prop_map, y = field, color = best_quartile, group = best_quartile,
            xmin = prop_low, xmax = prop_high)) +
      ggplot2::geom_point(position = position_dodge(width = 0.4)) +
      geom_linerange(position = position_dodge(width = 0.4)) +
      scale_y_discrete(limits = rev) +
      scale_x_sqrt(breaks = c(10, 20, 30, 40, 50),
                   limits = c(0, 60)) +
      facet_wrap(~disc, strip.position="right") +
      theme(axis.title.x = element_blank(),
            axis.title.y = element_blank())
    layout <- c(patchwork::area(t = 1, l = 1, b = 2, r = 3),
                patchwork::area(t = 3, l = 1, b = 12, r = 3),
                patchwork::area(t = 1, l = 4, b = 4, r = 5),
                patchwork::area(t = 5, l = 4, b = 12, r = 5))
    this_plot <- bio_plot + med_plot + psy_plot + biomed_plot +
      patchwork::plot_layout(design = layout, guides = 'collect') &
      theme(legend.position = 'top',
            legend.direction = 'horizontal',
            legend.background = element_rect(colour = "grey50"))
    ggplot2::ggsave(filename = 'plot_anova_subdisc_proportions.jpg',
                    plot = this_plot,
                    width = 12,
                    height = 8,
                    path = here::here('figures'),
                    dpi = 600)
    print(this_plot)
  }
}
plot_anova_subdisc_proportions()
```



```{r plot_anova_disc_count_diffs, echo = FALSE, message = FALSE}
plot_anova_disc_count_diffs <- function(load = TRUE){
  if(file.exists(here::here('figures', 'plot_anova_disc_count_diffs.jpg')) & load == TRUE){
    knitr::include_graphics(here::here('figures', 'plot_anova_disc_count_diffs.jpg'))
  }
  else{
    data <- data.table::fread(here::here('data/estimates', 'anova_count_diffs.txt'),
                              sep = '\t')
    this_plot <-
      ggplot2::ggplot(data = data[level == 'disc' &
                                    #best_quartile != 'Q4' &
                                    !((diff_map >0 & diff_low <0)|
                                        diff_map <0 & diff_high >0)],
                      aes(x = diff_map, y = disc,
                          color = best_quartile, group = best_quartile,
                          xmin = diff_low, xmax = diff_high)) +
      ggplot2::geom_vline(xintercept = 0, colour = 'black') +
      ggplot2::geom_point(position = position_dodge(width = 0.4)) +
      geom_linerange(position = position_dodge(width = 0.4)) +
      scale_y_discrete(limits = rev) +
      xlab('') +
      theme(legend.position = 'top', legend.direction = 'horizontal',
            legend.background = element_rect(colour = "grey50")) +
      ggh4x::facet_grid2(disc_group~forcats::fct_rev(anova_type),
                         scales = 'free', space = 'free_y',
                         independent = 'x')
    ggplot2::ggsave(filename = 'plot_anova_disc_count_diffs.jpg',
                    plot = this_plot,
                    width = 12,
                    height = 8,
                    path = here::here('figures'),
                    dpi = 600)
    print(this_plot)
  }
}

plot_anova_disc_count_diffs()
```

```{r plot_anova_subdisc_count_diffs, echo = FALSE, message = FALSE}
plot_anova_subdisc_count_diffs <- function(load = TRUE){
  if(file.exists(here::here('figures', 'plot_anova_subdisc_count_diffs.jpg')) & load == TRUE){
    knitr::include_graphics(here::here('figures', 'plot_anova_subdisc_count_diffs.jpg'))
  }
  else{
    data <- data.table::fread(here::here('data/estimates', 'anova_count_diffs.txt'),
                              sep = '\t') %>%
      .[, field := stringr::str_replace_all(field, 'ogy', '.')] %>%
      .[, field := stringr::str_replace(field, 'General', 'Gen.')] %>%
      .[, field := stringr::str_replace(field, 'Clinical', 'Clin.')] %>%
      .[, field := stringr::str_replace(field, 'medical ', 'med.')] %>%
      .[, field := stringr::str_replace(field, 'Medicine', 'Med.')] %>%
      .[, field := stringr::str_replace(field, 'mental ', 'm.')] %>%
      .[, field := stringr::str_replace(field, 'Science', 'Sc.')] %>%
      .[, field := stringr::str_replace(field, 'mentary ', 'm.')] %>%
      .[, field := stringr::str_replace(field, 'pational ', 'p.')] %>%
      .[, field := stringr::str_replace(field, 'Diseases?', 'Dis.')] %>%
      .[, field := stringr::str_replace(field, 'System', 'Sys.')] %>%
      .[, field := stringr::str_replace(field, 'chemistry', 'chem.')] %>%
      .[, field := stringr::str_replace(field, 'iratory', '.')] %>%
      .[, field := stringr::str_replace(field, 'ular ', '.')] %>%
      .[, field := stringr::str_replace(field, 'ioral ', '.')]
    
bio_plot <- 
      ggplot2::ggplot(
        data = data[disc %in% 'Biology'&
                      #best_quartile != 'Q4' &
                      anova_type == 'parametric' &
                      level == 'subdisc'  &
                      !((diff_map >0 & diff_low <0)|
                          diff_map <0 & diff_high >0)],
        aes(x = diff_map, y = field, color = best_quartile, group = best_quartile,
            xmin = diff_low, xmax = diff_high)) +
      ggplot2::geom_point(position = position_dodge(width = 0.4)) +
      ggplot2::geom_vline(xintercept = 0, colour = 'black') +
      geom_linerange(position = position_dodge(width = 0.4)) +
      scale_y_discrete(limits = rev) +
      facet_wrap(~disc, strip.position="right") +
      theme(axis.title.x = element_blank(),
            axis.title.y = element_blank())
    biomed_plot <- 
      ggplot2::ggplot(
        data = data[disc %in% 'Biomedical Research' &
                      #best_quartile != 'Q4' &
                      anova_type == 'parametric' &
                      level == 'subdisc' &
                      !((diff_map >0 & diff_low <0)|
                          diff_map <0 & diff_high >0)],
        aes(x = diff_map, y = field, color = best_quartile, group = best_quartile,
            xmin = diff_low, xmax = diff_high)) +
      ggplot2::geom_point(position = position_dodge(width = 0.4)) +
      ggplot2::geom_vline(xintercept = 0, colour = 'black') +
      geom_linerange(position = position_dodge(width = 0.4)) +
      scale_y_discrete(limits = rev) +
      facet_wrap(~disc, strip.position="right") +
      theme(axis.title.x = element_blank(),
            axis.title.y = element_blank())
    med_plot <- 
      ggplot2::ggplot(
        data = data[disc %in% 'Clinical Medicine'&
                      #best_quartile != 'Q4' &
                      anova_type == 'parametric' &
                      level == 'subdisc'&
                      !((diff_map >0 & diff_low <0)|
                          diff_map <0 & diff_high >0)],
        aes(x = diff_map, y = field, color = best_quartile, group = best_quartile,
            xmin = diff_low, xmax = diff_high)) +
      ggplot2::geom_point(position = position_dodge(width = 0.4)) +
      ggplot2::geom_vline(xintercept = 0, colour = 'black') +
      geom_linerange(position = position_dodge(width = 0.4)) +
      scale_y_discrete(limits = rev) +
      facet_wrap(~disc, strip.position="right") +
      theme(axis.title.x = element_blank(),
            axis.title.y = element_blank())
    psy_plot <- 
      ggplot2::ggplot(
        data = data[disc %in% 'Psychology'&
                      #best_quartile != 'Q4' &
                      anova_type == 'parametric' &
                      level == 'subdisc'&
                      !((diff_map >0 & diff_low <0)|
                          diff_map <0 & diff_high >0)],
        aes(x = diff_map, y = field, color = best_quartile, group = best_quartile,
            xmin = diff_low, xmax = diff_high)) +
      ggplot2::geom_point(position = position_dodge(width = 0.4)) +
      ggplot2::geom_vline(xintercept = 0, colour = 'black') +
      geom_linerange(position = position_dodge(width = 0.4)) +
      scale_y_discrete(limits = rev) +
      facet_wrap(~disc, strip.position="right") +
      theme(axis.title.x = element_blank(),
            axis.title.y = element_blank())
    layout <- c(patchwork::area(t = 1, l = 1, b = 2, r = 3),
                patchwork::area(t = 3, l = 1, b = 12, r = 3),
                patchwork::area(t = 1, l = 4, b = 4, r = 5),
                patchwork::area(t = 5, l = 4, b = 12, r = 5))
    this_plot <- bio_plot + med_plot + psy_plot + biomed_plot +
      patchwork::plot_layout(design = layout, guides = 'collect') &
      theme(legend.position = 'top',
            legend.direction = 'horizontal',
            legend.background = element_rect(colour = "grey50"))
    ggplot2::ggsave(filename = 'plot_anova_subdisc_count_diffs.jpg',
                    plot = this_plot,
                    width = 12,
                    height = 8,
                    path = here::here('figures'),
                    dpi = 600)
    print(this_plot)
  }
}

plot_anova_subdisc_count_diffs()
```

## Assumption Tests

```{r plot_assumption_disc_proportions, echo = FALSE, message = FALSE}
plot_assumption_disc_proportions <- function(load = TRUE){
  if(file.exists(here::here('figures', 'plot_assumption_disc_proportions.jpg')) & load == TRUE){
    knitr::include_graphics(here::here('figures', 'plot_assumption_disc_proportions.jpg'))
  }
  else{
    data <- data.table::fread(here::here('data/estimates', 'assumption_proportions.txt'),
                              sep = '\t')
    this_plot <-
      ggplot2::ggplot(data = data[level == 'disc'# &
                                    #best_quartile != 'Q4'
                                  ],
                      aes(x = prop_map, y = field, color = best_quartile,
                          shape = assumption_test,
                          xmin = prop_low, xmax = prop_high)) +
      ggplot2::geom_point(position = position_dodge(width = 0.75), size = 2) +
      geom_linerange(position = position_dodge(width = 0.75)) +
      scale_y_discrete(limits = rev) +
      xlab('%') +
      theme(axis.title.y = element_blank(),
            axis.title.x = element_text(face = 'bold'),
            legend.position = 'top',
            legend.direction = 'horizontal',
            legend.background = element_rect(colour = "grey50")) +
      ggh4x::facet_grid2(disc_group~forcats::fct_rev(anova_type),
                         scales = 'free', space = 'free_y',
                         independent = 'x')
    ggplot2::ggsave(filename = 'plot_assumption_disc_proportions.jpg',
                    plot = this_plot,
                    width = 12,
                    height = 8,
                    path = here::here('figures'),
                    dpi = 600)
    print(this_plot)
  }
}
plot_assumption_disc_proportions()
```

```{r plot_assumption_subdisc_proportions, echo = FALSE, message = FALSE}
plot_assumption_subdisc_proportions <- function(load = TRUE){
  if(file.exists(here::here('figures', 'plot_assumption_subdisc_proportions.jpg')) & load == TRUE){
    knitr::include_graphics(here::here('figures', 'plot_assumption_subdisc_proportions.jpg'))
  }
  else{
    data <- data.table::fread(here::here('data/estimates', 'assumption_proportions.txt'),
                              sep = '\t') %>%
      .[, field := stringr::str_replace_all(field, 'ogy', '.')] %>%
      .[, field := stringr::str_replace(field, 'General', 'Gen.')] %>%
      .[, field := stringr::str_replace(field, 'Clinical', 'Clin.')] %>%
      .[, field := stringr::str_replace(field, 'medical ', 'med.')] %>%
      .[, field := stringr::str_replace(field, 'Medicine', 'Med.')] %>%
      .[, field := stringr::str_replace(field, 'mental ', 'm.')] %>%
      .[, field := stringr::str_replace(field, 'Science', 'Sc.')] %>%
      .[, field := stringr::str_replace(field, 'mentary ', 'm.')] %>%
      .[, field := stringr::str_replace(field, 'pational ', 'p.')] %>%
      .[, field := stringr::str_replace(field, 'Diseases?', 'Dis.')] %>%
      .[, field := stringr::str_replace(field, 'System', 'Sys.')] %>%
      .[, field := stringr::str_replace(field, 'chemistry', 'chem.')] %>%
      .[, field := stringr::str_replace(field, 'iratory', '.')] %>%
      .[, field := stringr::str_replace(field, 'ular ', '.')] %>%
      .[, field := stringr::str_replace(field, 'ioral ', '.')]
    
    top_other_subdiscs <-
      data[level == 'subdisc' & !disc %in% c('Biology', 'Psychology')] %>%
      .[, .(max_prop = max(prop_map)),
        by = .(subdisc, disc_group)] %>%
      .[order(max_prop, decreasing = TRUE)] %>%
      .[, .(subdisc = head(subdisc, n = 5))]
    bio_plot <- 
      ggplot2::ggplot(
        data = data[disc %in% 'Biology'&
                      best_quartile != 'Q4' &
                      anova_type == 'parametric' &
                      assumption_test == 'multiplicity' &
                      level == 'subdisc'],
        aes(x = prop_map, y = field, color = best_quartile, group = best_quartile,
            xmin = prop_low, xmax = prop_high)) +
      ggplot2::geom_point(position = position_dodge(width = 0.4)) +
      geom_linerange(position = position_dodge(width = 0.4)) +
      scale_y_discrete(limits = rev) +
      scale_x_sqrt(limits = c(0, 90),
                   breaks = c(20, 40, 60, 80)) +
      facet_wrap(~disc, strip.position="right") +
      theme(axis.title.x = element_blank(),
            axis.title.y = element_blank())
    biomed_plot <- 
      ggplot2::ggplot(
        data = data[disc %in% 'Biomedical Research' &
                      best_quartile != 'Q4' &
                      anova_type == 'parametric' &
                      assumption_test == 'multiplicity' &
                      level == 'subdisc'],
        aes(x = prop_map, y = field, color = best_quartile, group = best_quartile,
            xmin = prop_low, xmax = prop_high)) +
      ggplot2::geom_point(position = position_dodge(width = 0.4)) +
      geom_linerange(position = position_dodge(width = 0.4)) +
      scale_y_discrete(limits = rev) +
      scale_x_sqrt(limits = c(0, 90),
                   breaks = c(20, 40, 60, 80)) +
      facet_wrap(~disc, strip.position="right") +
      theme(axis.title.x = element_blank(),
            axis.title.y = element_blank())
    med_plot <- 
      ggplot2::ggplot(
        data = data[disc %in% 'Clinical Medicine'&
                      best_quartile != 'Q4' &
                      anova_type == 'parametric' &
                      assumption_test == 'multiplicity' &
                      level == 'subdisc'],
        aes(x = prop_map, y = field, color = best_quartile, group = best_quartile,
            xmin = prop_low, xmax = prop_high)) +
      ggplot2::geom_point(position = position_dodge(width = 0.4)) +
      geom_linerange(position = position_dodge(width = 0.4)) +
      scale_y_discrete(limits = rev) +
      scale_x_sqrt(limits = c(0, 90),
                   breaks = c(20, 40, 60, 80)) +
      facet_wrap(~disc, strip.position="right") +
      theme(axis.title.x = element_blank(),
            axis.title.y = element_blank())
    psy_plot <- 
      ggplot2::ggplot(
        data = data[disc %in% 'Psychology'&
                      best_quartile != 'Q4' &
                      anova_type == 'parametric' &
                      assumption_test == 'multiplicity' &
                      level == 'subdisc'],
        aes(x = prop_map, y = field, color = best_quartile, group = best_quartile,
            xmin = prop_low, xmax = prop_high)) +
      ggplot2::geom_point(position = position_dodge(width = 0.4)) +
      geom_linerange(position = position_dodge(width = 0.4)) +
      scale_y_discrete(limits = rev) +
      scale_x_sqrt(limits = c(0, 90),
                   breaks = c(20, 40, 60, 80)) +
      facet_wrap(~disc, strip.position="right") +
      theme(axis.title.x = element_blank(),
            axis.title.y = element_blank())
    layout <- c(patchwork::area(t = 1, l = 1, b = 2, r = 3),
                patchwork::area(t = 3, l = 1, b = 12, r = 3),
                patchwork::area(t = 1, l = 4, b = 4, r = 5),
                patchwork::area(t = 5, l = 4, b = 12, r = 5))
    this_plot <- bio_plot + med_plot + psy_plot + biomed_plot +
      patchwork::plot_layout(design = layout, guides = 'collect') &
      theme(legend.position = 'top',
            legend.direction = 'horizontal',
            legend.background = element_rect(colour = "grey50"))
    ggplot2::ggsave(filename = 'plot_assumption_subdisc_proportions.jpg',
                    plot = this_plot,
                    width = 12,
                    height = 8,
                    path = here::here('figures'),
                    dpi = 600)
    print(this_plot)
  }
}
plot_assumption_subdisc_proportions()
```

```{r plot_assumption_disc_count_diffs, echo = FALSE, message = FALSE}

plot_assumption_disc_count_diffs <- function(load = TRUE){
  if(file.exists(here::here('figures', 'plot_assumption_disc_count_diffs.jpg')) & load == TRUE){
    knitr::include_graphics(here::here('figures', 'plot_assumption_disc_count_diffs.jpg'))
  }
  else{
    data <- data.table::fread(here::here('data/estimates', 'assumption_count_diffs.txt'),
                              sep = '\t')
    this_plot <-
      ggplot2::ggplot(data = data[level == 'disc' & best_quartile != 'Q4' &
                                    !((diff_map >0 & diff_low <0)|
                                        diff_map <0 & diff_high >0)],
                      aes(x = diff_map, y = field, color = best_quartile,
                          shape = assumption_test,group = best_quartile,
                          xmin = diff_low, xmax = diff_high)) +
      ggplot2::geom_vline(xintercept = 0, colour = 'black') +
      ggplot2::geom_point(position = position_dodge(width = 0.75), size = 2) +
      geom_linerange(position = position_dodge(width = 0.75)) +
      scale_y_discrete(limits = rev) +
      #scale_x_continuous(trans = scales::pseudo_log_trans(),
                         #breaks = c(-100, -50, -10, -5,
                                    #0, 5, 10, 50, 100),
                         #expand = c(0, 0)) +
      theme(legend.position = 'top', legend.direction = 'horizontal',
            legend.background = element_rect(colour = "grey50")) +
      ggh4x::facet_grid2(disc_group~forcats::fct_rev(anova_type),
                         scales = 'free', space = 'free_y',
                         independent = 'x')
    ggplot2::ggsave(filename = 'plot_assumption_disc_count_diffs.jpg',
                    plot = this_plot,
                    width = 12,
                    height = 8,
                    path = here::here('figures'),
                    dpi = 600)
    print(this_plot)
  }
}

plot_assumption_disc_count_diffs()
```


```{r plot_assumption_subdisc_count_diffs, echo = FALSE, message = FALSE}

plot_assumption_subdisc_count_diffs <- function(load = TRUE){
  if(file.exists(here::here('figures', 'plot_assumption_subdisc_count_diffs.jpg')) &
     load == TRUE){
    knitr::include_graphics(here::here('figures', 'plot_assumption_subdisc_count_diffs.jpg'))
  }
  else{
    data <- data.table::fread(here::here('data/estimates', 'assumption_count_diffs.txt'),
                              sep = '\t') %>%
      .[, field := stringr::str_replace_all(field, 'ogy', '.')] %>%
      .[, field := stringr::str_replace(field, 'General', 'Gen.')] %>%
      .[, field := stringr::str_replace(field, 'Clinical', 'Clin.')] %>%
      .[, field := stringr::str_replace(field, 'medical ', 'med.')] %>%
      .[, field := stringr::str_replace(field, 'Medicine', 'Med.')] %>%
      .[, field := stringr::str_replace(field, 'mental ', 'm.')] %>%
      .[, field := stringr::str_replace(field, 'Science', 'Sc.')] %>%
      .[, field := stringr::str_replace(field, 'mentary ', 'm.')] %>%
      .[, field := stringr::str_replace(field, 'pational ', 'p.')] %>%
      .[, field := stringr::str_replace(field, 'Diseases?', 'Dis.')] %>%
      .[, field := stringr::str_replace(field, 'System', 'Sys.')] %>%
      .[, field := stringr::str_replace(field, 'chemistry', 'chem.')] %>%
      .[, field := stringr::str_replace(field, 'iratory', '.')] %>%
      .[, field := stringr::str_replace(field, 'ular ', '.')] %>%
      .[, field := stringr::str_replace(field, 'ioral ', '.')]
    
bio_plot <- 
      ggplot2::ggplot(
        data = data[disc %in% 'Biology'&
                      best_quartile != 'Q4' &
                      anova_type == 'parametric' &
                      assumption_test == 'multiplicity' &
                      level == 'subdisc'],
        aes(x = diff_map, y = field, color = best_quartile, group = best_quartile,
            xmin = diff_low, xmax = diff_high)) +
      ggplot2::geom_point(position = position_dodge(width = 0.4)) +
      ggplot2::geom_vline(xintercept = 0, colour = 'black') +
      geom_linerange(position = position_dodge(width = 0.4)) +
      scale_y_discrete(limits = rev) +
      facet_wrap(~disc, strip.position="right") +
      theme(axis.title.x = element_blank(),
            axis.title.y = element_blank())
    biomed_plot <- 
      ggplot2::ggplot(
        data = data[disc %in% 'Biomedical Research' &
                      best_quartile != 'Q4' &
                      anova_type == 'parametric' &
                      assumption_test == 'multiplicity' &
                      level == 'subdisc'],
        aes(x = diff_map, y = field, color = best_quartile, group = best_quartile,
            xmin = diff_low, xmax = diff_high)) +
      ggplot2::geom_point(position = position_dodge(width = 0.4)) +
      ggplot2::geom_vline(xintercept = 0, colour = 'black') +
      geom_linerange(position = position_dodge(width = 0.4)) +
      scale_y_discrete(limits = rev) +
      facet_wrap(~disc, strip.position="right") +
      theme(axis.title.x = element_blank(),
            axis.title.y = element_blank())
    med_plot <- 
      ggplot2::ggplot(
        data = data[disc %in% 'Clinical Medicine'&
                      best_quartile != 'Q4' &
                      anova_type == 'parametric' &
                      assumption_test == 'multiplicity' &
                      level == 'subdisc'],
        aes(x = diff_map, y = field, color = best_quartile, group = best_quartile,
            xmin = diff_low, xmax = diff_high)) +
      ggplot2::geom_point(position = position_dodge(width = 0.4)) +
      ggplot2::geom_vline(xintercept = 0, colour = 'black') +
      geom_linerange(position = position_dodge(width = 0.4)) +
      scale_y_discrete(limits = rev) +
      facet_wrap(~disc, strip.position="right") +
      theme(axis.title.x = element_blank(),
            axis.title.y = element_blank())
    psy_plot <- 
      ggplot2::ggplot(
        data = data[disc %in% 'Psychology'&
                      best_quartile != 'Q4' &
                      anova_type == 'parametric' &
                      assumption_test == 'multiplicity' &
                      level == 'subdisc'],
        aes(x = diff_map, y = field, color = best_quartile, group = best_quartile,
            xmin = diff_low, xmax = diff_high)) +
      ggplot2::geom_point(position = position_dodge(width = 0.4)) +
      ggplot2::geom_vline(xintercept = 0, colour = 'black') +
      geom_linerange(position = position_dodge(width = 0.4)) +
      scale_y_discrete(limits = rev) +
      facet_wrap(~disc, strip.position="right") +
      theme(axis.title.x = element_blank(),
            axis.title.y = element_blank())
    layout <- c(patchwork::area(t = 1, l = 1, b = 2, r = 3),
                patchwork::area(t = 3, l = 1, b = 12, r = 3),
                patchwork::area(t = 1, l = 4, b = 4, r = 5),
                patchwork::area(t = 5, l = 4, b = 12, r = 5))
    this_plot <- bio_plot + med_plot + psy_plot + biomed_plot +
      patchwork::plot_layout(design = layout, guides = 'collect') &
      theme(legend.position = 'top',
            legend.direction = 'horizontal',
            legend.background = element_rect(colour = "grey50"))
    ggplot2::ggsave(filename = 'plot_assumption_subdisc_count_diffs.jpg',
                    plot = this_plot,
                    width = 12,
                    height = 8,
                    path = here::here('figures'),
                    dpi = 600)
    print(this_plot)
  }
}

plot_assumption_subdisc_count_diffs()
```

# Discussion

## Limitations

Use of ANOVA-related expressions does not necessarily imply use of
analysis of variance techniques. Indeed, authors may refer to analysis
of variance for specific reasons without actually using the technique.
In such cases, assuming that such mentions imply actual use would result
either in false positives or negatives, depending whether or not
mentions are also made of the assumption tests investigated here. While
this possibility cannot be ruled out, it is reasonable to think that
given the size of the dataset, such a possibility would be marginal at
best and wouldn't affect the overal trends observed in this study.

Restrictions and guidelines regarding article format and content might
have impacted statistical transparency. For example, editorial
directives regarding text length might have forced authors to cut short
on the description of statistical procedures in order to meet word count
requirements. Here again, while this situation remains a possibility, we
believe the size and disciplinary ubiquity of the current dataset goes a
long way towards neutralizing whatever effect this phenomenon might
have.

# References
