---
title: "A3_Classification"
output: html_document
date: "2022-08-18"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

pacman::p_load(
  tidyverse,
  brms
)
```

# The assignment
The Machine Learning assignment has 3 main parts: First we create a skeptical and an informed simulation, based on the meta-analysis. Second we build and test our machine learning pipeline on the simulated data. Second we apply the pipeline to the empirical data.

The report for the exam, thus, consists of the answer to all the following prompts:
- Describe your machine learning pipeline. Produce a diagram of it to guide the reader (e.g. see Rybner et al 2022 Vocal markers of autism: Assessing the generalizability of ML models), and describe the different parts: data budgeting, data preprocessing, model choice and training, assessment of performance.
- Briefly justify and describe your use of simulated data, and results from the pipeline on them.
- Describe results from applying the ML pipeline to the empirical data and what can we learn from them.

## Part I - Simulating data

Use the meta-analysis reported in Parola et al (2020), create a simulated dataset with 100 matched pairs of schizophrenia and controls, each participant producing 10 repeated measures (10 trials with their speech recorded). for each of these "recordings" (data points) produce 10 acoustic measures: 6 from the meta-analysis, 4 with just random noise. Do the same for a baseline dataset including only 10 noise variables. Tip: see the slides for the code. 

## Part II - ML pipeline on simulated data

On the two simulated datasets (separately) build a machine learning pipeline: i) create a data budget (e.g. balanced training and test sets); ii) pre-process the data (e.g. scaling the features); iii) fit and assess a classification algorithm on the training data (e.g. Bayesian multilevel logistic regression); iv) assess performance on the test set; v) discuss whether performance is as expected and feature importance is as expected.

Bonus question: replace the bayesian multilevel regression with a different algorithm, e.g. SVM or random forest (but really, anything you'd like to try).

## Part III - Applying the ML pipeline to empirical data

Download the empirical dataset from brightspace and apply your ML pipeline to the new data. Warning: in the simulated dataset we only had 10 features, now you have many more! Such is the life of the ML practitioner. Consider the impact a higher number of features will have on your ML inference, and decide whether you need to cut down the number of features before running the pipeline (or alternatively expand the pipeline to add feature selection).

```{r}

## First we define our population size
n <- 100
trials <- 10

## Then we define the different effect sizes: 6 from the m-a, 4 just random noise
InformedEffectMean <- c(0.25, -0.55, -0.75, -1.26, 0.05, 1.89, 0, 0, 0, 0)
SkepticEffectMean <- rep(0, 10)

# Then we define individual variability from population and across trials and measurement error.
IndividualSD <- 1
TrialSD <- 0.5
Error <- 0.2

## Then we setup the simulation

### For each pair of participants we need to identify the true effect size for each variable
for (i in seq(10)) {
  temp_informed <- tibble(
    ID = seq(n),
    TrueEffect = rnorm(n, InformedEffectMean[i], IndividualSD),
    Variable = paste0("v",i))
  temp_skeptic <- tibble(
    ID = seq(n),
    TrueEffect = rnorm(n, SkepticEffectMean[i], IndividualSD),
    Variable = paste0("v",i))
  if (i == 1) {
    d_informed_true <- temp_informed
    d_skeptic_true <- temp_skeptic
  } else {
      d_informed_true  <- rbind(d_informed_true, temp_informed)
      d_skeptic_true  <- rbind(d_skeptic_true, temp_skeptic)
    }
}

# Create tibble with one row per trial
d_trial <- tibble(expand_grid(ID = seq(n), Trial = seq(trials), Group = c("Schizophrenia", "Control")))

d_informed <- merge(d_informed_true, d_trial)
d_skeptic <- merge(d_skeptic_true, d_trial)

for (i in seq(nrow(d_informed))) {
  d_informed$measurement[i] <- ifelse(d_informed$Group[i] == "Schizophrenia",
                             rnorm(1, rnorm(1, d_informed$TrueEffect[i]/2, TrialSD), Error),
                             rnorm(1, rnorm(1, (-d_informed$TrueEffect[i])/2, TrialSD), Error))
  d_skeptic$measurement[i] <- ifelse(d_skeptic$Group[i] == "Schizophrenia",
                             rnorm(1, rnorm(1, d_skeptic$TrueEffect[i]/2, TrialSD), Error),
                             rnorm(1, rnorm(1, (-d_skeptic$TrueEffect[i])/2, TrialSD), Error))

  }
                  
d_informed_wide <- d_informed %>% mutate(TrueEffect = NULL) %>% pivot_wider(names_from = Variable, values_from = measurement)
d_skeptic_wide <- d_skeptic %>% mutate(TrueEffect = NULL) %>% pivot_wider(names_from = Variable, values_from = measurement)

d_informed_wide <- d_informed_wide %>%
  mutate(
    ID = as.factor(ID),
    Trial = as.factor(Trial),
    Group = as.factor(Group)
  )

d_skeptic_wide <- d_skeptic_wide %>%
  mutate(
    ID = as.factor(ID),
    Trial = as.factor(Trial),
    Group = as.factor(Group)
  )


ggplot(d_informed, aes(measurement, group = Group, fill = Group)) +
  geom_density(alpha = 0.3) +
  facet_wrap(.~Variable) +
  theme_bw()

ggplot(d_skeptic, aes(measurement, group = Group, fill = Group)) +
  geom_density(alpha = 0.3) +
  facet_wrap(.~Variable) +
  theme_bw()
```


# Part 2 - Setting up the pipeline

## Data budgeting

```{r}

TestID <- sample(seq(n), 20)

train_informed <- d_informed_wide %>% subset(!(ID %in% TestID))
test_informed <- d_informed_wide %>% subset(ID %in% TestID)

train_skeptic <- d_skeptic_wide %>% subset(!(ID %in% TestID))
test_skeptic <- d_skeptic_wide %>% subset(ID %in% TestID)

m1 <- mean(train_informed$v1)
sd1 <- sd(train_informed$v1)
train_informed <- train_informed %>% mutate(
  v1_s = (v1 - m1) / sd1
)
test_informed <- test_informed %>% mutate(
  v1_s = (v1 - m1) / sd1
)

library(tidymodels)

rec_informed <- train_informed %>%
  recipe(Group ~ . ) %>% # defines the outcome        
  step_scale(all_numeric() ) %>% # scales numeric predictors
  step_center(all_numeric() ) %>% # center numeric predictors
  prep(training = train_informed, retain = TRUE)

rec_skeptic <- train_skeptic %>%
  recipe(Group ~ . ) %>% # defines the outcome        
  step_scale(all_numeric() ) %>% # scales numeric predictors
  step_center(all_numeric() ) %>% # center numeric predictors
  prep(training = train_skeptic, retain = TRUE)


#Apply recipe to train and test
train_informed_s <- juice(rec_informed)
test_informed_s <- bake(rec_informed, new_data = test_informed, all_predictors()) %>% 
  mutate(Group = test_informed$Group)
train_skeptic_s <- juice(rec_skeptic)
test_skeptic_s <- bake(rec_skeptic, new_data = test_skeptic, all_predictors()) %>% 
  mutate(Group = test_skeptic$Group)

```

## Model fitting

```{r}
pacman::p_load(ggbump)

d <- train_informed_s %>% subset(Trial == 1)

ggplot(d, aes(v2, Group, colour = Group)) +
  geom_point() +
  theme_bw()

ggplot() +
  geom_point(data = d, aes(v2, Group, colour = Group)) +
  geom_smooth(data = d, aes(v2, as.numeric(Group)), method = lm) +
  theme_bw()

d %>% mutate(Group = as.numeric(Group) - 1) %>%
  ggplot() +
  geom_point(aes(v2, Group, colour = Group)) +
  geom_smooth(aes(v2, Group), method = "glm", method.args = list(family = "binomial")) +
  theme_bw()

PitchRange_f0 <- bf(Group ~ 1 + v2)

get_prior(PitchRange_f0, d, family = bernoulli)

plot_d <- tibble(
  value = c(inv_logit_scaled(rnorm(1e4, 0, 1)),
            inv_logit_scaled(rnorm(1e4, 0 + rnorm(1e4, 0, 3), 1))),
  predictor = rep(c("Intercept", "FullModel"), each = 1e4)
)

ggplot(plot_d, aes(value, group = predictor, fill = predictor)) +
  geom_histogram(aes(y = stat(density), group = predictor), alpha = 0.3) +
  theme_bw()

PitchRange_p0 <- c(
  prior(normal(0, 1), class = Intercept),
  prior(normal(0, .3), class = b)
)

PitchRange_m0 <- brm(
  PitchRange_f0,
  train_informed_s,
  family = bernoulli,
  prior = PitchRange_p0,
  sample_prior = T,
  backend = "cmdstanr",
  iter = 4e3,
  chains = 2,
  cores = 2,
  threads = threading(2),
  control = list(adapt_delta = 0.9,
                 max_treedepth = 20),
  stan_model_args = list(stanc_options = list("O1"))
)
pp_check(PitchRange_m0)

post <- as_draws_df(PitchRange_m0)

p1 <- ggplot(post) +
  geom_histogram(aes(b_Intercept), alpha = 0.5, fill = "green") +
  geom_histogram(aes(prior_Intercept), alpha = 0.5, fill = "red") +
  theme_bw()

p2 <- ggplot(post) +
  geom_histogram(aes(b_v2), alpha = 0.5, fill = "green") +
  geom_histogram(aes(prior_b), alpha = 0.5, fill = "red") +
  theme_bw()

p1 / p2

conditional_effects(PitchRange_m0)

## Now with varying effects
PitchRange_f1 <- bf(Group ~ 1 + v2 + (1 | ID))

get_prior(PitchRange_f1, d, family = bernoulli)

plot_d <- tibble(
  value = c(inv_logit_scaled(rnorm(1e4, 0, 1)),
            inv_logit_scaled(rnorm(1e4, 0 + rnorm(1e4, 0, 3), 1))),
  predictor = rep(c("Intercept", "FullModel"), each = 1e4)
)

ggplot(plot_d, aes(value, group = predictor, fill = predictor)) +
  geom_histogram(aes(y = stat(density), group = predictor), alpha = 0.3) +
  theme_bw()

PitchRange_p1 <- c(
  prior(normal(0, 1), class = Intercept),
  prior(normal(0, .3), class = b),
  prior(normal(0, .3), class = sd)
)

PitchRange_m1 <- brm(
  PitchRange_f1,
  train_informed_s,
  family = bernoulli,
  prior = PitchRange_p1,
  sample_prior = T,
  backend = "cmdstanr",
  iter = 4e3,
  chains = 2,
  cores = 2,
  threads = threading(2),
  control = list(adapt_delta = 0.9,
                 max_treedepth = 20),
  stan_model_args = list(stanc_options = list("O1"))
)

pp_check(PitchRange_m1)

post <- as_draws_df(PitchRange_m1)

p1 <- ggplot(post) +
  geom_histogram(aes(b_Intercept), alpha = 0.5, fill = "green") +
  geom_histogram(aes(prior_Intercept), alpha = 0.5, fill = "red") +
  theme_bw()

p2 <- ggplot(post) +
  geom_histogram(aes(b_v2), alpha = 0.5, fill = "green") +
  geom_histogram(aes(prior_b), alpha = 0.5, fill = "red") +
  theme_bw()

p3 <- ggplot(post) +
  geom_histogram(aes(sd_ID__Intercept), alpha = 0.5, fill = "green") +
  #geom_histogram(aes(sd_ID__Visit), alpha = 0.5, fill = "green") +
  geom_histogram(aes(prior_sd_ID), alpha = 0.5, fill = "red") +
  theme_bw()

p1 / p2 / p3
PitchRange_m1
conditional_effects(PitchRange_m1)


### Now also with Pitch Range
PitchRange_f2 <- bf(Group ~ 1 + v2 + (1 + v2 | ID))

get_prior(PitchRange_f1, d, family = bernoulli)

plot_d <- tibble(
  value = c(inv_logit_scaled(rnorm(1e4, 0, 1)),
            inv_logit_scaled(rnorm(1e4, 0 + rnorm(1e4, 0, 3), 1))),
  predictor = rep(c("Intercept", "FullModel"), each = 1e4)
)

ggplot(plot_d, aes(value, group = predictor, fill = predictor)) +
  geom_histogram(aes(y = stat(density), group = predictor), alpha = 0.3) +
  theme_bw()

PitchRange_p2 <- c(
  prior(normal(0, 1), class = Intercept),
  prior(normal(0, 1), class = b),
  prior(normal(0, 1), class = sd),
  prior(lkj(3), class = cor)
)

PitchRange_m2 <- brm(
  PitchRange_f2,
  train_informed_s,
  family = bernoulli,
  prior = PitchRange_p2,
  sample_prior = T,
  backend = "cmdstanr",
  chains = 2,
  cores = 2,
  iter = 4e3,
  threads = threading(2),
  control = list(adapt_delta = 0.9,
                 max_treedepth = 20),
  stan_model_args = list(stanc_options = list("O1"))
)

pp_check(PitchRange_m2)

post <- as_draws_df(PitchRange_m2)

p1 <- ggplot(post) +
  geom_histogram(aes(b_Intercept), alpha = 0.5, fill = "green") +
  geom_histogram(aes(prior_Intercept), alpha = 0.5, fill = "red") +
  theme_bw()

p2 <- ggplot(post) +
  geom_histogram(aes(b_v2), alpha = 0.5, fill = "green") +
  geom_histogram(aes(prior_b), alpha = 0.5, fill = "red") +
  theme_bw()

p3 <- ggplot(post) +
  geom_histogram(aes(sd_ID__Intercept), alpha = 0.5, fill = "green") +
  geom_histogram(aes(sd_ID__v2), alpha = 0.5, fill = "green") +
  geom_histogram(aes(prior_sd_ID), alpha = 0.5, fill = "red") +
  theme_bw()

p4 <- ggplot(post) +
  geom_histogram(aes(cor_ID__Intercept__v2), alpha = 0.5, fill = "green") +
  geom_histogram(aes(prior_cor_ID), alpha = 0.5, fill = "red") +
  theme_bw()

(p1 + p2) / (p3 + p4)
PitchRange_m2
conditional_effects(PitchRange_m2)

PitchRange_m0 <- add_criterion(PitchRange_m0, criterion = "loo")
PitchRange_m1 <- add_criterion(PitchRange_m1, criterion = "loo")
PitchRange_m2 <- add_criterion(PitchRange_m2, criterion = "loo")

loo_compare(PitchRange_m0, PitchRange_m1, PitchRange_m2)
loo_model_weights(PitchRange_m0, PitchRange_m1, PitchRange_m2)

PitchRange_m0_sk <- update(PitchRange_m0, newdata = train_skeptic_s)
PitchRange_m1_sk <- update(PitchRange_m1, newdata = train_skeptic_s)
PitchRange_m2_sk <- update(PitchRange_m2, newdata = train_skeptic_s)

train_informed_s$PredictionsPerc0 <- predict(PitchRange_m0)[, 1]
train_informed_s$Predictions0[train_informed_s$PredictionsPerc0 > 0.5] <- "Schizophrenia"
train_informed_s$Predictions0[train_informed_s$PredictionsPerc0 <= 0.5] <- "Control"

train_informed_s$PredictionsPerc1 <- predict(PitchRange_m1)[, 1]
train_informed_s$Predictions1[train_informed_s$PredictionsPerc1 > 0.5] <- "Schizophrenia"
train_informed_s$Predictions1[train_informed_s$PredictionsPerc1 <= 0.5] <- "Control"

train_informed_s$PredictionsPerc2 <- predict(PitchRange_m2)[, 1]
train_informed_s$Predictions2[train_informed_s$PredictionsPerc2 > 0.5] <- "Schizophrenia"
train_informed_s$Predictions2[train_informed_s$PredictionsPerc2 <= 0.5] <- "Control"

train_informed_s <- train_informed_s %>%
  mutate(
    Group = as.factor(Group),
    Predictions0 = as.factor(Predictions0),
    Predictions1 = as.factor(Predictions1),
    Predictions2 = as.factor(Predictions2)
  )

conf_mat(
  train_informed_s,
  truth = Group,
  estimate = Predictions0,
  dnn = c("Prediction", "Truth")
) 

metrics(train_informed_s, 
        truth = Group, estimate = Predictions0) %>% 
  knitr::kable()

conf_mat(
  train_informed_s,
  truth = Group,
  estimate = Predictions1,
  dnn = c("Prediction", "Truth")
) 

metrics(train_informed_s, 
        truth = Group, estimate = Predictions1) %>% 
  knitr::kable()

conf_mat(
  train_informed_s,
  truth = Group,
  estimate = Predictions2,
  dnn = c("Prediction", "Truth")
) 

metrics(train_informed_s, 
        truth = Group, estimate = Predictions2) %>% 
  knitr::kable()

test_informed_s$PredictionsPerc0 <- predict(PitchRange_m0, newdata = test_informed_s, allow_new_levels = T)[, 1]
test_informed_s$Predictions0[test_informed_s$PredictionsPerc0 > 0.5] <- "Schizophrenia"
test_informed_s$Predictions0[test_informed_s$PredictionsPerc0 <= 0.5] <- "Control"

test_informed_s$PredictionsPerc1 <- predict(PitchRange_m1, newdata = test_informed_s, allow_new_levels = T)[, 1]
test_informed_s$Predictions1[test_informed_s$PredictionsPerc1 > 0.5] <- "Schizophrenia"
test_informed_s$Predictions1[test_informed_s$PredictionsPerc1 <= 0.5] <- "Control"

test_informed_s$PredictionsPerc2 <- predict(PitchRange_m2, newdata = test_informed_s, allow_new_levels = T)[, 1]
test_informed_s$Predictions2[test_informed_s$PredictionsPerc2 > 0.5] <- "Schizophrenia"
test_informed_s$Predictions2[test_informed_s$PredictionsPerc2 <= 0.5] <- "Control"


test_informed_s <- test_informed_s %>%
  mutate(
    Group = as.factor(Group),
    Predictions0 = as.factor(Predictions0),
    Predictions1 = as.factor(Predictions1),
    Predictions2 = as.factor(Predictions2)
  )


conf_mat(
  test_informed_s,
  truth = Group,
  estimate = Predictions0,
  dnn = c("Prediction", "Truth")
) 


metrics(test_informed_s, 
        truth = Group, estimate = Predictions0) %>% 
  knitr::kable()

conf_mat(
  test_informed_s,
  truth = Group,
  estimate = Predictions1,
  dnn = c("Prediction", "Truth")
) 

metrics(test_informed_s, 
        truth = Group, estimate = Predictions1) %>% 
  knitr::kable()

conf_mat(
  test_informed_s,
  truth = Group,
  estimate = Predictions2,
  dnn = c("Prediction", "Truth")
) 

metrics(test_informed_s, 
        truth = Group, estimate = Predictions2) %>% 
  knitr::kable()


Performance <- tibble(
  Model = c("FixedEffects", "VaryingIntercept", "VaryingSlope", "FixedEffects", "VaryingIntercept", "VaryingSlope","FixedEffects", "VaryingIntercept", "VaryingSlope", "FixedEffects", "VaryingIntercept", "VaryingSlope"),
  Setup = c("informed", "informed","informed","informed","informed","informed",
            "skeptic", "skeptic", "skeptic", "skeptic", "skeptic", "skeptic"),
  Type = c("training", "training", "training", "test", "test", "test",
           "training", "training", "training", "test", "test", "test"),
  Accuracy = c(
    accuracy(train_informed_s, truth = Group, estimate = Predictions0)[, ".estimate"],
    accuracy(train_informed_s, truth = Group, estimate = Predictions1)[, ".estimate"],
    accuracy(train_informed_s, truth = Group, estimate = Predictions2)[, ".estimate"],
    accuracy(test_informed_s, truth = Group, estimate = Predictions0)[, ".estimate"],
    accuracy(test_informed_s, truth = Group, estimate = Predictions1)[, ".estimate"],
    accuracy(test_informed_s, truth = Group, estimate = Predictions2)[, ".estimate"],
    accuracy(train_skeptic_s, truth = Group, estimate = Predictions0)[, ".estimate"],
    accuracy(train_skeptic_s, truth = Group, estimate = Predictions1)[, ".estimate"],
    accuracy(train_skeptic_s, truth = Group, estimate = Predictions2)[, ".estimate"],
    accuracy(test_skeptic_s, truth = Group, estimate = Predictions0)[, ".estimate"],
    accuracy(test_skeptic_s, truth = Group, estimate = Predictions1)[, ".estimate"],
    accuracy(test_skeptic_s, truth = Group, estimate = Predictions2)[, ".estimate"]
    ),
  BalancedAccuracy = c(
    bal_accuracy(train_informed_s, truth = Group, estimate = Predictions0)[, ".estimate"],
    bal_accuracy(train_informed_s, truth = Group, estimate = Predictions1)[, ".estimate"],
    bal_accuracy(train_informed_s, truth = Group, estimate = Predictions2)[, ".estimate"],
    bal_accuracy(test_informed_s, truth = Group, estimate = Predictions0)[, ".estimate"],
    bal_accuracy(test_informed_s, truth = Group, estimate = Predictions1)[, ".estimate"],
    bal_accuracy(test_informed_s, truth = Group, estimate = Predictions2)[, ".estimate"],
    bal_accuracy(train_skeptic_s, truth = Group, estimate = Predictions0)[, ".estimate"],
    bal_accuracy(train_skeptic_s, truth = Group, estimate = Predictions1)[, ".estimate"],
    bal_accuracy(train_skeptic_s, truth = Group, estimate = Predictions2)[, ".estimate"],
    bal_accuracy(test_skeptic_s, truth = Group, estimate = Predictions0)[, ".estimate"],
    bal_accuracy(test_skeptic_s, truth = Group, estimate = Predictions1)[, ".estimate"],
    bal_accuracy(test_skeptic_s, truth = Group, estimate = Predictions2)[, ".estimate"]
    ),
  F1 = c(
    f_meas(train_informed_s, truth = Group, estimate = Predictions0)[, ".estimate"],
    f_meas(train_informed_s, truth = Group, estimate = Predictions1)[, ".estimate"],
    f_meas(train_informed_s, truth = Group, estimate = Predictions2)[, ".estimate"],
    f_meas(test_informed_s, truth = Group, estimate = Predictions0)[, ".estimate"],
    f_meas(test_informed_s, truth = Group, estimate = Predictions1)[, ".estimate"],
    f_meas(test_informed_s, truth = Group, estimate = Predictions2)[, ".estimate"],
    f_meas(train_skeptic_s, truth = Group, estimate = Predictions0)[, ".estimate"],
    f_meas(train_skeptic_s, truth = Group, estimate = Predictions1)[, ".estimate"],
    f_meas(train_skeptic_s, truth = Group, estimate = Predictions2)[, ".estimate"],
    f_meas(test_skeptic_s, truth = Group, estimate = Predictions0)[, ".estimate"],
    f_meas(test_skeptic_s, truth = Group, estimate = Predictions1)[, ".estimate"],
    f_meas(test_skeptic_s, truth = Group, estimate = Predictions2)[, ".estimate"]
    )
) %>% mutate(Accuracy = as.numeric(Accuracy), BalancedAccuracy = as.numeric(BalancedAccuracy), F1 = as.numeric(F1))

ggplot(Performance, aes(Model, F1, group = Type, color = Type)) +
  geom_point() +
  geom_line() +
  geom_hline(yintercept = 0.5) +
  facet_grid(.~Setup) +
  theme_bw()

ggplot(Performance, aes(Model, BalancedAccuracy, group = Type, color = Type)) +
  geom_point() +
  geom_line() +
  facet_grid(.~Setup) +
  theme_bw()

sensitivity(train_informed_s, 
        truth = Group, estimate = Predictions0)
sensitivity(train_informed_s, 
        truth = Group, estimate = Predictions1)
sensitivity(train_informed_s, 
        truth = Group, estimate = Predictions2)

sensitivity(test_informed_s, 
        truth = Group, estimate = Predictions0)
sensitivity(test_informed_s, 
        truth = Group, estimate = Predictions1)
sensitivity(test_informed_s, 
        truth = Group, estimate = Predictions2)

specificity(train_informed_s, 
        truth = Group, estimate = Predictions0)
specificity(train_informed_s, 
        truth = Group, estimate = Predictions1)
specificity(train_informed_s, 
        truth = Group, estimate = Predictions2)

specificity(test_informed_s, 
        truth = Group, estimate = Predictions0)
specificity(test_informed_s, 
        truth = Group, estimate = Predictions1)
specificity(test_informed_s, 
        truth = Group, estimate = Predictions2)

ppv(train_informed_s, 
        truth = Group, estimate = Predictions0)
ppv(train_informed_s, 
        truth = Group, estimate = Predictions1)
ppv(train_informed_s, 
        truth = Group, estimate = Predictions2)

ppv(test_informed_s, 
        truth = Group, estimate = Predictions0)
ppv(test_informed_s, 
        truth = Group, estimate = Predictions1)
ppv(test_informed_s, 
        truth = Group, estimate = Predictions2)

npv(train_informed_s, 
        truth = Group, estimate = Predictions0)
npv(train_informed_s, 
        truth = Group, estimate = Predictions1)
npv(train_informed_s, 
        truth = Group, estimate = Predictions2)

npv(test_informed_s, 
        truth = Group, estimate = Predictions0)
npv(test_informed_s, 
        truth = Group, estimate = Predictions1)
npv(test_informed_s, 
        truth = Group, estimate = Predictions2)

bal_accuracy(train_informed_s, 
        truth = Group, estimate = Predictions0)
bal_accuracy(train_informed_s, 
        truth = Group, estimate = Predictions1)
bal_accuracy(train_informed_s, 
        truth = Group, estimate = Predictions2)

bal_accuracy(test_informed_s, 
        truth = Group, estimate = Predictions0)
bal_accuracy(test_informed_s, 
        truth = Group, estimate = Predictions1)
bal_accuracy(test_informed_s, 
        truth = Group, estimate = Predictions2)

f_meas(train_informed_s, 
        truth = Group, estimate = Predictions0)
f_meas(train_informed_s, 
        truth = Group, estimate = Predictions1)
f_meas(train_informed_s, 
        truth = Group, estimate = Predictions2)

test_informed_s$PredictionsPerc0 <- predict(PitchRange_m0, newdata = test_informed_s, allow_new_levels = T)[, 1]
test_informed_s$Predictions0[test_informed_s$PredictionsPerc0 > 0.5] <- "Schizophrenia"
test_informed_s$Predictions0[test_informed_s$PredictionsPerc0 <= 0.5] <- "Control"
f_meas(test_informed_s, 
        truth = Group, estimate = Predictions0)
f_meas(test_informed_s, 
        truth = Group, estimate = Predictions1)
f_meas(test_informed_s, 
        truth = Group, estimate = Predictions2)

train_skeptic_s$PredictionsPerc0 <- predict(PitchRange_m0_sk)[, 1]
train_skeptic_s$Predictions0[train_skeptic_s$PredictionsPerc0 > 0.5] <- "Schizophrenia"
train_skeptic_s$Predictions0[train_skeptic_s$PredictionsPerc0 <= 0.5] <- "Control"

train_skeptic_s$PredictionsPerc1 <- predict(PitchRange_m1_sk)[, 1]
train_skeptic_s$Predictions1[train_skeptic_s$PredictionsPerc1 > 0.5] <- "Schizophrenia"
train_skeptic_s$Predictions1[train_skeptic_s$PredictionsPerc1 <= 0.5] <- "Control"

train_skeptic_s$PredictionsPerc2 <- predict(PitchRange_m2_sk)[, 1]
train_skeptic_s$Predictions2[train_skeptic_s$PredictionsPerc2 > 0.5] <- "Schizophrenia"
train_skeptic_s$Predictions2[train_skeptic_s$PredictionsPerc2 <= 0.5] <- "Control"

train_skeptic_s <- train_skeptic_s %>%
  mutate(
    Group = as.factor(Group),
    Predictions0 = as.factor(Predictions0),
    Predictions1 = as.factor(Predictions1),
    Predictions2 = as.factor(Predictions2)
  )

conf_mat(
  train_skeptic_s,
  truth = Group,
  estimate = Predictions0,
  dnn = c("Prediction", "Truth")
) 

metrics(train_skeptic_s, 
        truth = Group, estimate = Predictions0) %>% 
  knitr::kable()

conf_mat(
  train_skeptic_s,
  truth = Group,
  estimate = Predictions1,
  dnn = c("Prediction", "Truth")
) 

metrics(train_skeptic_s, 
        truth = Group, estimate = Predictions1) %>% 
  knitr::kable()

conf_mat(
  train_skeptic_s,
  truth = Group,
  estimate = Predictions2,
  dnn = c("Prediction", "Truth")
) 

metrics(train_skeptic_s, 
        truth = Group, estimate = Predictions2) %>% 
  knitr::kable()

test_skeptic_s$PredictionsPerc0 <- predict(PitchRange_m0_sk, newdata = test_skeptic_s, allow_new_levels = T)[, 1]
test_skeptic_s$Predictions0[test_skeptic_s$PredictionsPerc0 > 0.5] <- "Schizophrenia"
test_skeptic_s$Predictions0[test_skeptic_s$PredictionsPerc0 <= 0.5] <- "Control"

test_skeptic_s$PredictionsPerc1 <- predict(PitchRange_m1_sk, newdata = test_skeptic_s, allow_new_levels = T)[, 1]
test_skeptic_s$Predictions1[test_skeptic_s$PredictionsPerc1 > 0.5] <- "Schizophrenia"
test_skeptic_s$Predictions1[test_skeptic_s$PredictionsPerc1 <= 0.5] <- "Control"

test_skeptic_s$PredictionsPerc2 <- predict(PitchRange_m2_sk, newdata = test_skeptic_s, allow_new_levels = T)[, 1]
test_skeptic_s$Predictions2[test_skeptic_s$PredictionsPerc2 > 0.5] <- "Schizophrenia"
test_skeptic_s$Predictions2[test_skeptic_s$PredictionsPerc2 <= 0.5] <- "Control"


test_skeptic_s <- test_skeptic_s %>%
  mutate(
    Group = as.factor(Group),
    Predictions0 = as.factor(Predictions0),
    Predictions1 = as.factor(Predictions1),
    Predictions2 = as.factor(Predictions2)
  )


conf_mat(
  test_skeptic_s,
  truth = Group,
  estimate = Predictions0,
  dnn = c("Prediction", "Truth")
) 

metrics(test_skeptic_s, 
        truth = Group, estimate = Predictions0) %>% 
  knitr::kable()

conf_mat(
  test_skeptic_s,
  truth = Group,
  estimate = Predictions1,
  dnn = c("Prediction", "Truth")
) 

metrics(test_skeptic_s, 
        truth = Group, estimate = Predictions1) %>% 
  knitr::kable()

conf_mat(
  test_skeptic_s,
  truth = Group,
  estimate = Predictions2,
  dnn = c("Prediction", "Truth")
) 

metrics(test_skeptic_s, 
        truth = Group, estimate = Predictions2) %>% 
  knitr::kable()

sensitivity(train_skeptic_s, 
        truth = Group, estimate = Predictions0)
sensitivity(train_skeptic_s, 
        truth = Group, estimate = Predictions1)
sensitivity(train_skeptic_s, 
        truth = Group, estimate = Predictions2)

sensitivity(test_skeptic_s, 
        truth = Group, estimate = Predictions0)
sensitivity(test_skeptic_s, 
        truth = Group, estimate = Predictions1)
sensitivity(test_skeptic_s, 
        truth = Group, estimate = Predictions2)

specificity(train_skeptic_s, 
        truth = Group, estimate = Predictions0)
specificity(train_skeptic_s, 
        truth = Group, estimate = Predictions1)
specificity(train_skeptic_s, 
        truth = Group, estimate = Predictions2)

specificity(test_skeptic_s, 
        truth = Group, estimate = Predictions0)
specificity(test_skeptic_s, 
        truth = Group, estimate = Predictions1)
specificity(test_skeptic_s, 
        truth = Group, estimate = Predictions2)

ppv(train_skeptic_s, 
        truth = Group, estimate = Predictions0)
ppv(train_skeptic_s, 
        truth = Group, estimate = Predictions1)
ppv(train_skeptic_s, 
        truth = Group, estimate = Predictions2)

ppv(test_skeptic_s, 
        truth = Group, estimate = Predictions0)
ppv(test_skeptic_s, 
        truth = Group, estimate = Predictions1)
ppv(test_skeptic_s, 
        truth = Group, estimate = Predictions2)

npv(train_skeptic_s, 
        truth = Group, estimate = Predictions0)
npv(train_skeptic_s, 
        truth = Group, estimate = Predictions1)
npv(train_skeptic_s, 
        truth = Group, estimate = Predictions2)

npv(test_skeptic_s, 
        truth = Group, estimate = Predictions0)
npv(test_skeptic_s, 
        truth = Group, estimate = Predictions1)
npv(test_skeptic_s, 
        truth = Group, estimate = Predictions2)

bal_accuracy(train_skeptic_s, 
        truth = Group, estimate = Predictions0)
bal_accuracy(train_skeptic_s, 
        truth = Group, estimate = Predictions1)
bal_accuracy(train_skeptic_s, 
        truth = Group, estimate = Predictions2)

bal_accuracy(test_skeptic_s, 
        truth = Group, estimate = Predictions0)
bal_accuracy(test_skeptic_s, 
        truth = Group, estimate = Predictions1)
bal_accuracy(test_skeptic_s, 
        truth = Group, estimate = Predictions2)

f_meas(train_skeptic_s, 
        truth = Group, estimate = Predictions0)
f_meas(train_skeptic_s, 
        truth = Group, estimate = Predictions1)
f_meas(train_skeptic_s, 
        truth = Group, estimate = Predictions2)

f_meas(test_skeptic_s, 
        truth = Group, estimate = Predictions0)
f_meas(test_skeptic_s, 
        truth = Group, estimate = Predictions1)
f_meas(test_skeptic_s, 
        truth = Group, estimate = Predictions2)

## Now with uncertainty
PerformanceProb <- tibble(expand_grid(
  Sample = seq(4000),
  Model = c("FixedEffects", "VaryingIntercept", "VaryingSlope"),
  Setup = c("informed", "skeptic"),
  Type = c("training", "test"))
)


train0 <- inv_logit_scaled(posterior_linpred(PitchRange_m0, summary = F)) # newdata = test_informed_s, allow_new_levels = T,
test0 <- inv_logit_scaled(posterior_linpred(PitchRange_m0, summary = F, newdata = test_informed_s, allow_new_levels = T))
train1 <- inv_logit_scaled(posterior_linpred(PitchRange_m1, summary = F)) # newdata = test_informed_s, allow_new_levels = T,
test1 <- inv_logit_scaled(posterior_linpred(PitchRange_m1, summary = F, newdata = test_informed_s, allow_new_levels = T))
train2 <- inv_logit_scaled(posterior_linpred(PitchRange_m2, summary = F)) # newdata = test_informed_s, allow_new_levels = T,
test2 <- inv_logit_scaled(posterior_linpred(PitchRange_m2, summary = F, newdata = test_informed_s, allow_new_levels = T))

train0_sk <- inv_logit_scaled(posterior_linpred(PitchRange_m0_sk, summary = F)) # newdata = test_informed_s, allow_new_levels = T,
test0_sk <- inv_logit_scaled(posterior_linpred(PitchRange_m0_sk, summary = F, newdata = test_skeptic_s, allow_new_levels = T))
train1_sk <- inv_logit_scaled(posterior_linpred(PitchRange_m1_sk, summary = F)) # newdata = test_informed_s, allow_new_levels = T,
test1_sk <- inv_logit_scaled(posterior_linpred(PitchRange_m1_sk, summary = F, newdata = test_skeptic_s, allow_new_levels = T))
train2_sk <- inv_logit_scaled(posterior_linpred(PitchRange_m2_sk, summary = F)) # newdata = test_informed_s, allow_new_levels = T,
test2_sk <- inv_logit_scaled(posterior_linpred(PitchRange_m2_sk, summary = F, newdata = test_skeptic_s, allow_new_levels = T))

for (i in seq(4000)) {
  
  train_informed_s$Predictions0 <- as.factor(ifelse(train0[i,] > 0.5, "Schizophrenia", "Control"))
  train_informed_s$Predictions1 <- as.factor(ifelse(train1[i,] > 0.5, "Schizophrenia", "Control"))
  train_informed_s$Predictions2 <- as.factor(ifelse(train2[i,] > 0.5, "Schizophrenia", "Control"))
  test_informed_s$Predictions0 <- as.factor(ifelse(test0[i,] > 0.5, "Schizophrenia", "Control"))
  test_informed_s$Predictions1 <- as.factor(ifelse(test1[i,] > 0.5, "Schizophrenia", "Control"))
  test_informed_s$Predictions2 <- as.factor(ifelse(test1[i,] > 0.5, "Schizophrenia", "Control"))
  
  train_skeptic_s$Predictions0 <- as.factor(ifelse(train0_sk[i,] > 0.5, "Schizophrenia", "Control"))
  train_skeptic_s$Predictions1 <- as.factor(ifelse(train1_sk[i,] > 0.5, "Schizophrenia", "Control"))
  train_skeptic_s$Predictions2 <- as.factor(ifelse(train2_sk[i,] > 0.5, "Schizophrenia", "Control"))
  test_skeptic_s$Predictions0 <- as.factor(ifelse(test0_sk[i,] > 0.5, "Schizophrenia", "Control"))
  test_skeptic_s$Predictions1 <- as.factor(ifelse(test1_sk[i,] > 0.5, "Schizophrenia", "Control"))
  test_skeptic_s$Predictions2 <- as.factor(ifelse(test2_sk[i,] > 0.5, "Schizophrenia", "Control"))
  
  PerformanceProb$Accuracy[PerformanceProb$Sample == i & PerformanceProb$Model == "FixedEffects" &
                             PerformanceProb$Setup == "informed" & PerformanceProb$Type == "training"] <-
    accuracy(train_informed_s, truth = Group, estimate = Predictions0)[, ".estimate"]
  PerformanceProb$Accuracy[PerformanceProb$Sample == i & PerformanceProb$Model == "VaryingIntercept" &
                             PerformanceProb$Setup == "informed" & PerformanceProb$Type == "training"] <-
    accuracy(train_informed_s, truth = Group, estimate = Predictions1)[, ".estimate"]
  PerformanceProb$Accuracy[PerformanceProb$Sample == i & PerformanceProb$Model == "VaryingSlope" &
                             PerformanceProb$Setup == "informed" & PerformanceProb$Type == "training"] <-
    accuracy(train_informed_s, truth = Group, estimate = Predictions2)[, ".estimate"]
  
  PerformanceProb$Accuracy[PerformanceProb$Sample == i & PerformanceProb$Model == "FixedEffects" &
                             PerformanceProb$Setup == "informed" & PerformanceProb$Type == "test"] <-
    accuracy(test_informed_s, truth = Group, estimate = Predictions0)[, ".estimate"]
  PerformanceProb$Accuracy[PerformanceProb$Sample == i & PerformanceProb$Model == "VaryingIntercept" &
                             PerformanceProb$Setup == "informed" & PerformanceProb$Type == "test"] <-
    accuracy(test_informed_s, truth = Group, estimate = Predictions1)[, ".estimate"]
  PerformanceProb$Accuracy[PerformanceProb$Sample == i & PerformanceProb$Model == "VaryingSlope" &
                             PerformanceProb$Setup == "informed" & PerformanceProb$Type == "test"] <-
    accuracy(test_informed_s, truth = Group, estimate = Predictions2)[, ".estimate"]
  
  PerformanceProb$Accuracy[PerformanceProb$Sample == i & PerformanceProb$Model == "FixedEffects" &
                             PerformanceProb$Setup == "skeptic" & PerformanceProb$Type == "training"] <-
    accuracy(train_skeptic_s, truth = Group, estimate = Predictions0)[, ".estimate"]
  PerformanceProb$Accuracy[PerformanceProb$Sample == i & PerformanceProb$Model == "VaryingIntercept" &
                             PerformanceProb$Setup == "skeptic" & PerformanceProb$Type == "training"] <-
    accuracy(train_skeptic_s, truth = Group, estimate = Predictions1)[, ".estimate"]
  PerformanceProb$Accuracy[PerformanceProb$Sample == i & PerformanceProb$Model == "VaryingSlope" &
                             PerformanceProb$Setup == "skeptic" & PerformanceProb$Type == "training"] <-
    accuracy(train_skeptic_s, truth = Group, estimate = Predictions2)[, ".estimate"]
  
  PerformanceProb$Accuracy[PerformanceProb$Sample == i & PerformanceProb$Model == "FixedEffects" &
                             PerformanceProb$Setup == "skeptic" & PerformanceProb$Type == "test"] <-
    accuracy(test_skeptic_s, truth = Group, estimate = Predictions0)[, ".estimate"]
  PerformanceProb$Accuracy[PerformanceProb$Sample == i & PerformanceProb$Model == "VaryingIntercept" &
                             PerformanceProb$Setup == "skeptic" & PerformanceProb$Type == "test"] <-
    accuracy(test_skeptic_s, truth = Group, estimate = Predictions1)[, ".estimate"]
  PerformanceProb$Accuracy[PerformanceProb$Sample == i & PerformanceProb$Model == "VaryingSlope" &
                             PerformanceProb$Setup == "skeptic" & PerformanceProb$Type == "test"] <-
    accuracy(test_skeptic_s, truth = Group, estimate = Predictions2)[, ".estimate"]
  
  ## ADD ROC !
  test_informed_s$perc = test0[i,]
  roc_curve(test_informed_s, truth = Group, estimate = perc) %>%
  ggplot(aes(x = 1 - specificity, y = sensitivity)) +
  geom_path() +
  geom_abline(lty = 3) +
  coord_equal() +
  theme_bw()
    
}

ggplot(PerformanceProb, aes(Model, Accuracy, group = Type, color = Type)) +
  geom_point() +
  #geom_line() +
  geom_hline(yintercept = 0.5) +
  facet_grid(.~Setup) +
  theme_bw()


```

## Extend to all features (feature selection? feature importance?)
```{r}
All_f0 <- bf(Group ~ 1 + v1 + v2 + v3 + v4 + v5 + v6 + v7 + v8 + v9 + v10)

All_f2 <- bf(Group ~ 1 + v1 + v2 + v3 + v4 + v5 + v6 + v7 + v8 + v9 + v10 + (1 + v1 + v2 + v3 + v4 + v5 + v6 + v7 + v8 + v9 + v10 | ID))

All_p0 <- c(
  brms::prior(normal(0, 1), class = Intercept),
  brms::prior(normal(0, 0.2), class = b)
)

All_p2 <- c(
  brms::prior(normal(0, 1), class = Intercept),
  brms::prior(normal(0, 0.2), class = b),
  brms::prior(normal(0, 1), class = sd),
  brms::prior(lkj(3), class = cor)
)

All_m0 <- brm(
  All_f0,
  train_informed_s,
  family = bernoulli,
  prior = All_p0,
  sample_prior = T,
  backend = "cmdstanr",
  chains = 2,
  cores = 2,
  iter = 4e3,
  threads = threading(2),
  control = list(adapt_delta = 0.9,
                 max_treedepth = 20),
  stan_model_args = list(stanc_options = list("O1"))
)

train_informed_s$Preds <- as.factor(ifelse(predict(All_m0)[,1] > 0.5, "Schizophrenia", "Control"))
test_informed_s$Preds <- as.factor(
  ifelse(predict(All_m0, newdata = test_informed_s, allow_new_levels = T)[,1] > 0.5, 
  "Schizophrenia", "Control"))

accuracy(train_informed_s, truth = Group, estimate = Preds)
accuracy(test_informed_s, truth = Group, estimate = Preds)

posterior <- as_draws_df(All_m0) %>% select(b_v1:b_v10)
p1 <- ggplot(posterior, aes(b_v1)) + geom_density(alpha = 0.3) + geom_vline(xintercept = 0.25, color = "red", linetype = "dotted") + theme_bw()

p2 <- ggplot(posterior, aes(b_v2)) + geom_density(alpha = 0.3) + geom_vline(xintercept = -0.55, color = "red", linetype = "dotted") + theme_bw()
p3 <- ggplot(posterior, aes(b_v3)) + geom_density(alpha = 0.3) + geom_vline(xintercept = -0.75, color = "red", linetype = "dotted") + theme_bw()
p4 <- ggplot(posterior, aes(b_v4)) + geom_density(alpha = 0.3) + geom_vline(xintercept = -1.26, color = "red", linetype = "dotted") + theme_bw()
p5 <- ggplot(posterior, aes(b_v5)) + geom_density(alpha = 0.3) + geom_vline(xintercept = 0.05, color = "red", linetype = "dotted") + theme_bw()
p6 <- ggplot(posterior, aes(b_v6)) + geom_density(alpha = 0.3) + geom_vline(xintercept = 1.89, color = "red", linetype = "dotted") + theme_bw()
p7 <- ggplot(posterior, aes(b_v7)) + geom_density(alpha = 0.3) + geom_vline(xintercept = 0, color = "red", linetype = "dotted") + theme_bw()
p8 <- ggplot(posterior, aes(b_v8)) + geom_density(alpha = 0.3) + geom_vline(xintercept = 0, color = "red", linetype = "dotted") + theme_bw()
p9 <- ggplot(posterior, aes(b_v9)) + geom_density(alpha = 0.3) + geom_vline(xintercept = 0, color = "red", linetype = "dotted") + theme_bw()
p10 <- ggplot(posterior, aes(b_v10)) + geom_density(alpha = 0.3) + geom_vline(xintercept = 0, color = "red", linetype = "dotted") + theme_bw()

library(patchwork)
(p1 + p2 + p3 + p4 + p5 + p6 + p7 + p8 + p9 + p10)

rank_data <- tibble(
  Variable = colnames(posterior),
  TrueEffect = InformedEffectMean,
  EstimatedMean = colMeans(posterior),
  LowCI = apply(posterior, 2, quantile, probs = 0.025, na.rm = TRUE),
  HighCI = apply(posterior, 2, quantile, probs = 0.975, na.rm = TRUE)
)


ggplot(rank_data, aes(TrueEffect, EstimatedMean)) + 
  geom_linerange(aes(ymin = LowCI, ymax = HighCI)) +
  geom_point() +
  geom_abline(intercept = 0, slope = 1, linetype = "dashed", color = "pink") +
  theme_bw()


All_m2 <- brm(
  All_f2,
  train_informed_s,
  family = bernoulli,
  #prior = All_p2,
  sample_prior = T,
  backend = "cmdstanr",
  chains = 2,
  cores = 2,
  iter = 4e3,
  threads = threading(2),
  control = list(adapt_delta = 0.9,
                 max_treedepth = 20),
  stan_model_args = list(stanc_options = list("O1"))
)

All_m2_sk <- brm(
  All_f2,
  train_skeptic_s,
  family = bernoulli,
  #prior = All_p2,
  sample_prior = T,
  backend = "cmdstanr",
  chains = 2,
  cores = 2,
  iter = 4e3,
  threads = threading(2),
  control = list(adapt_delta = 0.9,
                 max_treedepth = 20),
  stan_model_args = list(stanc_options = list("O1"))
)

# MISSING: Feature importance

# Assessing impact of prior 
All_p2 <- c(
  prior(normal(0, 1), class = Intercept),
  prior(normal(0, 0.2), class = b),
  prior(normal(0, 1), class = sd),
  prior(lkj(3), class = cor)
  )

# construct a sequence of sds to loop through for the slope prior:
priSD <- seq(0.1, 1.5, length.out = 15)
priorsN <- All_p2

#create empty variables to store output of the loop:
PerformanceProb <- tibble(expand_grid(
  Sample = seq(4000),
  Prior = priSD,
  Setup = c("informed", "skeptic"),
  Type = c("training", "test"))
)

for (i in 1:length(priSD)) {
  print(i)
  priorsN[2,] <- set_prior(paste0("normal(0, ", priSD[i],")"), class = "b")
  
  model_for_loop <- update(
    All_m2,
    prior = priorsN,
    sample_prior = T,
    backend = "cmdstanr",
    chains = 2,
    cores = 2,
    iter = 4e3,
    threads = threading(2),
    control = list(adapt_delta = 0.9,
                   max_treedepth = 20),
    stan_model_args = list(stanc_options = list("O1")))
  
    model_sk_for_loop <- update(
    All_m2_sk,
    prior = priorsN,
    sample_prior = T,
    backend = "cmdstanr",
    chains = 2,
    cores = 2,
    iter = 4e3,
    threads = threading(2),
    control = list(adapt_delta = 0.9,
                   max_treedepth = 20),
    stan_model_args = list(stanc_options = list("O1")))
  
    ## HERE INSTEAD WE NEED TO CALCULATE TRAIN AND TEST ACCURACY
    train <- inv_logit_scaled(posterior_linpred(model_for_loop, summary = F)) 
    test <- inv_logit_scaled(posterior_linpred(model_for_loop, summary = F, newdata = test_informed_s, allow_new_levels = T))
    train_sk <- inv_logit_scaled(posterior_linpred(model_sk_for_loop, summary = F)) 
    test_sk <- inv_logit_scaled(posterior_linpred(model_sk_for_loop, summary = F, newdata = test_skeptic_s, allow_new_levels = T))
    
    for (j in seq(4000)) {
      
      train_informed_s$Predictions <- as.factor(ifelse(train[j,] > 0.5, "Schizophrenia", "Control"))
      test_informed_s$Predictions <- as.factor(ifelse(test[j,] > 0.5, "Schizophrenia", "Control"))
      
      train_skeptic_s$Predictions <- as.factor(ifelse(train_sk[j,] > 0.5, "Schizophrenia", "Control"))
      test_skeptic_s$Predictions <- as.factor(ifelse(test_sk[j,] > 0.5, "Schizophrenia", "Control"))
      
      PerformanceProb$Accuracy[PerformanceProb$Sample == j & PerformanceProb$Prior == priSD[i] &
                                 PerformanceProb$Setup == "informed" & PerformanceProb$Type == "training"] <-
        accuracy(train_informed_s, truth = Group, estimate = Predictions)[, ".estimate"]
      
      PerformanceProb$Accuracy[PerformanceProb$Sample == j & PerformanceProb$Prior == priSD[i] &
                                 PerformanceProb$Setup == "informed" & PerformanceProb$Type == "test"] <-
        accuracy(test_informed_s, truth = Group, estimate = Predictions)[, ".estimate"]
      
      PerformanceProb$Accuracy[PerformanceProb$Sample == j & PerformanceProb$Prior == priSD[i] &
                                 PerformanceProb$Setup == "skeptic" & PerformanceProb$Type == "training"] <-
        accuracy(train_skeptic_s, truth = Group, estimate = Predictions)[, ".estimate"]
      
      PerformanceProb$Accuracy[PerformanceProb$Sample == j & PerformanceProb$Prior == priSD[i] &
                                 PerformanceProb$Setup == "skeptic" & PerformanceProb$Type == "test"] <-
        accuracy(test_skeptic_s, truth = Group, estimate = Predictions)[, ".estimate"]
    }
  
}

write_csv(PerformanceProb, "PriorSensitivity_ML.csv")

ggplot(PerformanceProb, aes(as.numeric(Prior), as.numeric(Accuracy), color = Type, group = Type)) +
  geom_point(alpha = 0.05) +
  labs(x = "Standard Deviation of Slope Prior", 
       y = "Accuracy", 
       title = "Sensitivity analysis for accuracy") +
  geom_hline(yintercept = 0.5) +
  facet_grid(.~ Setup) +
  theme_bw() 
  
  #+
  #theme(plot.title = element_text(hjust = 0.5, size = 15),
  #      axis.title.x = element_text(size = 13),
  #      axis.text.y = element_text(size = 12),
  #      axis.text.x = element_text(size = 12),
   #     axis.title.y = element_text(size = 13))


library(dplyr)     # for data wrangling
library(tidyr)     # same
library(purrr)     # for functional programming
library(rlang)     # for tidyeval
library(ggplot2)   # for dataviz
library(ggsci)     # for nice colours
library(scales)    # for displaying percentages
library(brms)      # for Bayesian models
library(tidybayes) # for extracting posterior draws and predictions
library(yardstick) # for generating ROC curves
# you might need to install cmdstanr too

# set options ------------------------------------------------------------------
options(mc.cores = 4, brms.backend = "cmdstanr") # for faster compilation and sampling
set.seed(888) # for reproducibility
theme_set(theme_ggdist()) # change ggplot theme

# create functions -------------------------------------------------------------

# generate mean posterior predictions and ROC values
get_roc_curve <- function(newdata, object, ...) {
    
    # enquote response variable and get brmsfit family
    resp_var <- formula(object)[["formula"]][[2]]
    resp_var <- enquo(resp_var)
    model_fam <- object[["family"]][["family"]]
    
    # object must be a brmsfit object with a supported family
    supported <- c("bernoulli", "binomial", "categorical", "cumulative", "sratio", "cratio", "acat")
    stopifnot(is.brmsfit(object))
    if (!(model_fam %in% supported)) stop(paste0("model family must be one of: ", paste0(supported, collapse = ", ")))
    
    if (model_fam %in% c("binomial", "bernoulli")) {
        roc_values <- add_epred_draws(newdata, object, ...) %>% 
            ungroup() %>% 
            mutate(!!resp_var := as.factor(!!resp_var)) %>% 
            # generate a ROC curve for each posterior draw
            split(.$.draw) %>%
            map_dfr(~roc_curve(., truth = !!resp_var, .epred, event_level = "second"), .id = ".draw") 
        
    } else {
        cat_symbols <- syms(as.character(unique(get_y(object))))
        roc_values <- add_epred_draws(newdata, object, ...) %>% 
            ungroup() %>% 
            mutate(!!resp_var := as.factor(!!resp_var)) %>%
            # spread predictions for different categories across different columns
            pivot_wider(names_from = .category, values_from = .epred) %>% 
            # generate a ROC curve for each posterior draw
            split(.$.draw) %>% 
            map_dfr(~roc_curve(., truth = !!resp_var, !!!cat_symbols), .id = ".draw")
    }
    
    return(roc_values)
}

roc <- get_roc_curve(test_informed_s, All_m0, ndraws = 50)


ggplot(roc, aes(1 - specificity, sensitivity))  +
    geom_line(aes(y = 1 - specificity), linetype = "dotted", colour = "black") +
    geom_line(aes(group = interaction(.draw)), alpha = 0.1) +
    stat_summary(fun = mean, geom = "line", size = 1) + # mean posterior prediction
    scale_colour_d3() +
    scale_x_continuous(labels = percent) +
    scale_y_continuous(labels = percent) +
    coord_equal() +
    labs(x = "1- Specificity", y = "Sensibility", colour = "Model") +
    theme_ggdist() +
    theme(legend.position = "none")
```


## Example of tidymodels on all features
```{r}
pacman::p_load(kernlab, randomForest, xgboost, knitr, dotwhisker, rstanarm)

d_inf <- train_informed_s %>% 
  mutate(ID = NULL, Trial = NULL, Preds = NULL, Predictions = NULL, v1_s = NULL)
d_sk <- train_skeptic_s %>% 
  mutate(ID = NULL, Trial = NULL, Preds = NULL, Predictions = NULL, v1_s = NULL)

LogisticRegression_inf <- logistic_reg() %>%
  set_mode("classification") %>% 
  set_engine("glm") %>%
  fit(Group ~ . , data = d_inf)
LogisticRegression_sk <- logistic_reg() %>%
  set_mode("classification") %>% 
  set_engine("glm") %>%
  fit(Group ~ . , data = d_sk)

SVM_inf <- svm_rbf() %>% 
  set_mode("classification") %>%	set_engine("kernlab") %>%
  fit(Group ~ . , data = d_inf)
SVM_sk <- svm_rbf() %>% 
  set_mode("classification") %>%	set_engine("kernlab") %>%
  fit(Group ~ . , data = d_sk)

RandomForest_inf <- rand_forest() %>%
  set_mode("classification") %>% 
  set_engine("randomForest") %>%
  fit(Group ~ . , data = d_inf)
RandomForest_sk <- rand_forest() %>%
  set_mode("classification") %>% 
  set_engine("randomForest") %>%
  fit(Group ~ . , data = d_sk)

BoostedTree_inf <- boost_tree() %>%
  set_mode("classification") %>% 
  set_engine("xgboost") %>%
  fit(Group ~ . , data = d_inf)

BoostedTree_sk <- boost_tree() %>%
  set_mode("classification") %>% 
  set_engine("xgboost") %>%
  fit(Group ~ . , data = d_sk)

Stan_sk <- logistic_reg() %>%
  set_mode("classification") %>% 
  set_engine("stan") %>%
  fit(Group ~ . , data = d_sk)

pacman::p_load(DALEX, DALEXtra)

explainer_lm <- 
  explain_tidymodels(
    LogisticRegression_inf, 
    data = train_informed_s, 
    y = as.numeric(train_informed_s$Group) - 1,
    label = "logReg",
    verbose = FALSE
  )

explainer_lm %>% model_parts() %>% plot(show_boxplots = FALSE) + ggtitle("Feature Importance ", "")

model_profile_lm1 <- model_profile(explainer_lm, type = "partial", 
                                   variables = c("v1", "v2", "v3", "v4", "v5", "v6"))
plot(model_profile_lm1, variables = c("v1", "v2", "v3", "v4", "v5", "v6")) + 
  ggtitle("Partial dependence profile ", "")


explainer_rf <- 
  explain_tidymodels(
    RandomForest_inf, 
    data = train_informed_s, 
    y = as.numeric(train_informed_s$Group) - 1,
    label = "random forest",
    verbose = FALSE
  )

explainer_rf %>% model_parts() %>% plot(show_boxplots = FALSE) + ggtitle("Feature Importance ", "")

model_profile_rf1 <- model_profile(explainer_rf, type = "partial", variables = c("v1", "v2", "v3", "v4", "v5", "v6"))

plot(model_profile_rf1, variables = c("v1", "v2", "v3", "v4", "v5", "v6")) + ggtitle("Partial dependence profile ", "")


inf_rs <- bootstraps(train_informed_s, times = 30)
svm_mod <-
  svm_rbf(cost = tune(), rbf_sigma = tune()) %>%
  set_mode("classification") %>%
  set_engine("kernlab")
roc_vals <- metric_set(roc_auc)
formula_res <-
  svm_mod %>% 
  tune_grid(
    Group ~ .,
    resamples = inf_rs,
    metrics = roc_vals
  )
estimates <- collect_metrics(formula_res)
tidy(LogisticRegression_inf) %>% 
  dwplot(dot_args = list(size = 2, color = "black"),
         whisker_args = list(color = "black"),
         vline = geom_vline(xintercept = 0, colour = "grey50", linetype = 2))

tidy(LogisticRegression_sk) %>% 
  dwplot(dot_args = list(size = 2, color = "black"),
         whisker_args = list(color = "black"),
         vline = geom_vline(xintercept = 0, colour = "grey50", linetype = 2))


informed_results0 <-  train_informed_s %>%
  as_tibble() %>%
  mutate(
    log_class = predict(LogisticRegression_inf, new_data = train_informed_s) %>% 
      pull(.pred_class), 
    log_prob  = predict(LogisticRegression_inf, new_data = train_informed_s, type = "prob") %>% 
      pull(.pred_Schizophrenia),
    svm_class = predict(SVM_inf, new_data = train_informed_s) %>% 
      pull(.pred_class),
    svm_prob  = predict(SVM_inf, new_data = train_informed_s, type = "prob") %>% 
      pull(.pred_Schizophrenia),
    rf_class = predict(RandomForest_inf, new_data = train_informed_s) %>% 
      pull(.pred_class),
    rf_prob  = predict(RandomForest_inf, new_data = train_informed_s, type = "prob") %>% 
      pull(.pred_Schizophrenia),
    bt_class = predict(BoostedTree_inf, new_data = train_informed_s) %>% 
      pull(.pred_class),
    bt_prob  = predict(BoostedTree_inf, new_data = train_informed_s, type = "prob") %>% 
      pull(.pred_Schizophrenia),
    )

informed_results <-  test_informed_s %>%
  as_tibble() %>%
  mutate(
    log_class = predict(LogisticRegression_inf, new_data = test_informed_s) %>% 
      pull(.pred_class), 
    log_prob  = predict(LogisticRegression_inf, new_data = test_informed_s, type = "prob") %>% 
      pull(.pred_Schizophrenia),
    svm_class = predict(SVM_inf, new_data = test_informed_s) %>% 
      pull(.pred_class),
    svm_prob  = predict(SVM_inf, new_data = test_informed_s, type = "prob") %>% 
      pull(.pred_Schizophrenia),
    rf_class = predict(RandomForest_inf, new_data = test_informed_s) %>% 
      pull(.pred_class),
    rf_prob  = predict(RandomForest_inf, new_data = test_informed_s, type = "prob") %>% 
      pull(.pred_Schizophrenia),
    bt_class = predict(BoostedTree_inf, new_data = test_informed_s) %>% 
      pull(.pred_class),
    bt_prob  = predict(BoostedTree_inf, new_data = test_informed_s, type = "prob") %>% 
      pull(.pred_Schizophrenia),
    )

skeptic_results0 <-  train_skeptic_s %>%
  as_tibble() %>%
  mutate(
    log_class = predict(LogisticRegression_sk, new_data = train_skeptic_s) %>% 
      pull(.pred_class), 
    log_prob  = predict(LogisticRegression_sk, new_data = train_skeptic_s, type = "prob") %>% 
      pull(.pred_Schizophrenia),
    svm_class = predict(SVM_sk, new_data = train_skeptic_s) %>% 
      pull(.pred_class),
    svm_prob  = predict(SVM_sk, new_data = train_skeptic_s, type = "prob") %>% 
      pull(.pred_Schizophrenia),
    rf_class = predict(RandomForest_sk, new_data = train_skeptic_s) %>% 
      pull(.pred_class),
    rf_prob  = predict(RandomForest_sk, new_data = train_skeptic_s, type = "prob") %>% 
      pull(.pred_Schizophrenia),
    bt_class = predict(BoostedTree_sk, new_data = train_skeptic_s) %>% 
      pull(.pred_class),
    bt_prob  = predict(BoostedTree_sk, new_data = train_skeptic_s, type = "prob") %>% 
      pull(.pred_Schizophrenia),
    )

skeptic_results <-  test_skeptic_s %>%
  as_tibble() %>%
  mutate(
    log_class = predict(LogisticRegression_sk, new_data = test_skeptic_s) %>% 
      pull(.pred_class), 
    log_prob  = predict(LogisticRegression_sk, new_data = test_skeptic_s, type = "prob") %>% 
      pull(.pred_Schizophrenia),
    svm_class = predict(SVM_sk, new_data = test_skeptic_s) %>% 
      pull(.pred_class),
    svm_prob  = predict(SVM_sk, new_data = test_skeptic_s, type = "prob") %>% 
      pull(.pred_Schizophrenia),
    rf_class = predict(RandomForest_sk, new_data = test_skeptic_s) %>% 
      pull(.pred_class),
    rf_prob  = predict(RandomForest_sk, new_data = test_skeptic_s, type = "prob") %>% 
      pull(.pred_Schizophrenia),
    bt_class = predict(BoostedTree_sk, new_data = test_skeptic_s) %>% 
      pull(.pred_class),
    bt_prob  = predict(BoostedTree_sk, new_data = test_skeptic_s, type = "prob") %>% 
      pull(.pred_Schizophrenia),
    )

metrics_d <- tibble(
  Model = c("Logistic", "SVM", "RF", "BT","Logistic", "SVM", "RF", "BT",
            "Logistic", "SVM", "RF", "BT","Logistic", "SVM", "RF", "BT"),
  Data = c("Train", "Train", "Train", "Train", "Test", "Test", "Test", "Test",
           "Train", "Train", "Train", "Train", "Test", "Test", "Test", "Test"),
  Type = c("Informed", "Informed", "Informed", "Informed", "Informed", "Informed", "Informed", "Informed",
           "Skeptic", "Skeptic", "Skeptic", "Skeptic", "Skeptic", "Skeptic", "Skeptic", "Skeptic"),
  Performance = c(
    metrics(informed_results0, truth = Group, estimate = log_class)[[3]][1],
    metrics(informed_results0, truth = Group, estimate = svm_class)[[3]][1],
    metrics(informed_results0, truth = Group, estimate = rf_class)[[3]][1],
    metrics(informed_results0, truth = Group, estimate = bt_class)[[3]][1],
    metrics(informed_results, truth = Group, estimate = log_class)[[3]][1],
    metrics(informed_results, truth = Group, estimate = svm_class)[[3]][1],
    metrics(informed_results, truth = Group, estimate = rf_class)[[3]][1],
    metrics(informed_results, truth = Group, estimate = bt_class)[[3]][1],
    metrics(skeptic_results0, truth = Group, estimate = log_class)[[3]][1],
    metrics(skeptic_results0, truth = Group, estimate = svm_class)[[3]][1],
    metrics(skeptic_results0, truth = Group, estimate = rf_class)[[3]][1],
    metrics(skeptic_results0, truth = Group, estimate = bt_class)[[3]][1],
    metrics(skeptic_results, truth = Group, estimate = log_class)[[3]][1],
    metrics(skeptic_results, truth = Group, estimate = svm_class)[[3]][1],
    metrics(skeptic_results, truth = Group, estimate = rf_class)[[3]][1],
    metrics(skeptic_results, truth = Group, estimate = bt_class)[[3]][1]
  )
)

ggplot(metrics_d, aes(Model, Performance, group = Data, color = Data)) +
  geom_point(alpha = 0.5) +
  facet_grid(.~ Type) +
  theme_bw()

#https://pbiecek.github.io/DALEX_docs/index.html#introduction




```

```{r}
pacman::p_load(tidyverse)
pacman::p_load(tidymodels)
pacman::p_load(caret)

d1 <- read_csv("Dropbox/Teaching/2022 - Methods 3/Assignments22/A3_ML/data/Ass3_empiricalData1.csv") %>%
  mutate(
    PatID = NULL,
    NewID = NULL,
    Language = NULL,
    Trial = NULL,
    Corpus = NULL,
    Gender = NULL,
    Diagnosis = as.factor(Diagnosis)
  )

d2 <- d1 %>% mutate(Diagnosis = NULL)

summary(d1)
res <-  cor(d1)
caret::findLinearCombos(d1)

rec_informed <- d1 %>%
  recipe(Diagnosis ~ . ) %>% # defines the outcome        
  step_scale(all_numeric() ) %>% # scales numeric predictors
  step_center(all_numeric() ) %>% # center numeric predictors
  prep(training = d1, retain = TRUE)


#Apply recipe to train and test
d1_s <- juice(d1)
d1$Diagnosis = as.factor(d1$Diagnosis)

LogisticRegression_inf <- logistic_reg() %>%
  set_mode("classification") %>% 
  set_engine("glm") %>%
  fit(Diagnosis ~ . , data = d1)
```


